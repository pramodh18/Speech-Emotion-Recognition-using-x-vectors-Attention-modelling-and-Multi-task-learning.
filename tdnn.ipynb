{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\MTP\n"
     ]
    }
   ],
   "source": [
    "cd F:/MTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.listdir('EmoDB/wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_code = {\n",
    "    'W':0, #anger\n",
    "    'L':1, #boredom\n",
    "    'E':2, #disgust\n",
    "    'A':3, #fear\n",
    "    'F':4, #happy\n",
    "    'T':5, #sad\n",
    "    'N':6  #neutral\n",
    "}\n",
    "\n",
    "speaker_code = {\n",
    "    '03':0,\n",
    "    '08':1,\n",
    "    '09':2,\n",
    "    '10':3,\n",
    "    '11':4,\n",
    "    '12':5,\n",
    "    '13':6,\n",
    "    '14':7,\n",
    "    '15':8,\n",
    "    '16':9\n",
    "}\n",
    "\n",
    "gender_code = {\n",
    "    '03':0,\n",
    "    '08':1,\n",
    "    '09':1,\n",
    "    '10':0,\n",
    "    '11':0,\n",
    "    '12':0,\n",
    "    '13':1,\n",
    "    '14':1,\n",
    "    '15':0,\n",
    "    '16':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_id = {0:\"03\",1:\"08\",2:\"09\",3:\"10\",4:\"11\",5:\"12\",6:\"13\",7:\"14\",8:\"15\",9:\"16\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir_path, test_key,valid_key, transform=None):\n",
    "        self.dir_path = dir_path\n",
    "        self.files = os.listdir(self.dir_path)\n",
    "        self.test_key = test_key\n",
    "        self.valid_key = valid_key\n",
    "        self.melspecs,self.Y = self.loadData(dir_path)\n",
    "        print(len(self.melspecs),len(self.Y))\n",
    "        \n",
    "    def loadData(self,dir_path):\n",
    "        files = os.listdir(dir_path)\n",
    "        train_keys = list((spk_id[i] for i in range(10) if i not in [self.test_key,self.valid_key]))\n",
    "        melspecs = []\n",
    "        Y = []\n",
    "        for key in train_keys:\n",
    "            for file in files:\n",
    "                if file[:2]==key:\n",
    "                    r, sr = librosa.load(dir_path + file, res_type='kaiser_fast')\n",
    "                    melspec = librosa.feature.melspectrogram(y=r, sr=sr,n_fft = 512, hop_length=160, win_length=320,n_mels=24)\n",
    "                    temp = self.chunk(torch.Tensor(melspec))\n",
    "                    melspecs.extend(temp)\n",
    "                    for _ in range(len(temp)):\n",
    "                        y = torch.zeros(7,dtype = int)\n",
    "                        y[emotion_code[file[5]]]=1\n",
    "                        Y.append(y)\n",
    "        return melspecs,Y\n",
    "    \n",
    "    def chunk(self,melspec):\n",
    "        melspec = melspec.transpose(0,1)\n",
    "        res = []\n",
    "        for i in range(0,melspec.size(0),50):\n",
    "            temp = melspec[i:i+100,:]\n",
    "            if temp.size(0)==100:\n",
    "                res.append(temp)\n",
    "        return res        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.melspecs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.melspecs[idx],self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2629 2629\n"
     ]
    }
   ],
   "source": [
    "ds = TrainDataset(\"EmoDB/wav/\",0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(ds, batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "658"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir_path,valid_key, transform=None):\n",
    "        self.dir_path = dir_path\n",
    "        self.files = os.listdir(self.dir_path)\n",
    "        self.valid_key = valid_key\n",
    "        self.melspecs,self.Y = self.loadData(dir_path)\n",
    "        print(len(self.melspecs),len(self.Y))\n",
    "        \n",
    "    def loadData(self,dir_path):\n",
    "        files = os.listdir(dir_path)\n",
    "        melspecs = []\n",
    "        Y = []\n",
    "        for file in files:\n",
    "            if file[:2]==spk_id[self.valid_key]:\n",
    "                r, sr = librosa.load(dir_path + file, res_type='kaiser_fast')\n",
    "                melspec = librosa.feature.melspectrogram(y=r, sr=sr,n_fft = 512, hop_length=160, win_length=320,n_mels=24)\n",
    "                temp = self.chunk(torch.Tensor(melspec))\n",
    "                melspecs.extend(temp)\n",
    "                for _ in range(len(temp)):\n",
    "                    y = torch.zeros(7,dtype = int)\n",
    "                    y[emotion_code[file[5]]]=1\n",
    "                    Y.append(y)\n",
    "        return melspecs,Y\n",
    "    \n",
    "    def chunk(self,melspec):\n",
    "        melspec = melspec.transpose(0,1)\n",
    "        res = []\n",
    "        for i in range(0,melspec.size(0),50):\n",
    "            temp = melspec[i:i+100,:]\n",
    "            if temp.size(0)==100:\n",
    "                res.append(temp)\n",
    "        return res        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.melspecs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.melspecs[idx],self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399 399\n"
     ]
    }
   ],
   "source": [
    "validset = ValidDataset(\"EmoDB/wav/\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(ds, batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "658"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir_path, test_key, transform=None):\n",
    "        self.dir_path = dir_path\n",
    "        self.files = os.listdir(self.dir_path)\n",
    "        self.test_key = test_key\n",
    "        self.melspecs,self.Y = self.loadData(dir_path)\n",
    "        print(len(self.melspecs),len(self.Y))\n",
    "        \n",
    "    def loadData(self,dir_path):\n",
    "        files = os.listdir(dir_path)\n",
    "        melspecs = []\n",
    "        Y = []\n",
    "        key = spk_id[self.test_key]\n",
    "        for file in files:\n",
    "            if file[:2]==key:\n",
    "                r, sr = librosa.load(dir_path + file, res_type='kaiser_fast')\n",
    "                melspec = librosa.feature.melspectrogram(y=r, sr=sr,n_fft = 512, hop_length=160, win_length=320,n_mels=24)\n",
    "                melspec = melspec.transpose()\n",
    "                melspecs.append(melspec)\n",
    "                y = torch.zeros(7,dtype = int)\n",
    "                y[emotion_code[file[5]]]=1\n",
    "                Y.append(y)\n",
    "        return melspecs,Y\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.melspecs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.melspecs[idx],self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    }
   ],
   "source": [
    "testset = TestDataset(\"EmoDB/wav/\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(testset, batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TDNN(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "                    self, \n",
    "                    input_dim=23, \n",
    "                    output_dim=512,\n",
    "                    context_size=5,\n",
    "                    stride=1,\n",
    "                    dilation=1,\n",
    "                    batch_norm=False,\n",
    "                    dropout_p=0.2\n",
    "                ):\n",
    "        super(TDNN, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.stride = stride\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dilation = dilation\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_norm = batch_norm\n",
    "      \n",
    "        self.kernel = nn.Linear(input_dim*context_size, output_dim)\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        if self.batch_norm:\n",
    "            self.bn = nn.BatchNorm1d(output_dim)\n",
    "        if self.dropout_p:\n",
    "            self.drop = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        _, _, d = x.shape\n",
    "        assert (d == self.input_dim), 'Input dimension was wrong. Expected ({}), got ({})'.format(self.input_dim, d)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Unfold input into smaller temporal contexts\n",
    "        x = F.unfold(\n",
    "                        x, \n",
    "                        (self.context_size, self.input_dim), \n",
    "                        stride=(1,self.input_dim), \n",
    "                        dilation=(self.dilation,1)\n",
    "                    )\n",
    "\n",
    "        # N, output_dim*context_size, new_t = x.shape\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.kernel(x.float())\n",
    "        x = self.nonlinearity(x)\n",
    "        \n",
    "        if self.dropout_p:\n",
    "            x = self.drop(x)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            x = x.transpose(1,2)\n",
    "            x = self.bn(x)\n",
    "            x = x.transpose(1,2)\n",
    "\n",
    "        return x\n",
    "import torch.nn as nn\n",
    "# from models.tdnn import TDNN\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class X_vector(nn.Module):\n",
    "    def __init__(self, input_dim = 24, num_classes=7):\n",
    "        super(X_vector, self).__init__()\n",
    "        self.tdnn1 = TDNN(input_dim=input_dim, output_dim=512, context_size=5, dilation=1,dropout_p=0.5)\n",
    "        self.tdnn2 = TDNN(input_dim=512, output_dim=512, context_size=3, dilation=1,dropout_p=0.5)\n",
    "        self.tdnn3 = TDNN(input_dim=512, output_dim=512, context_size=2, dilation=2,dropout_p=0.5)\n",
    "        self.tdnn4 = TDNN(input_dim=512, output_dim=512, context_size=1, dilation=1,dropout_p=0.5)\n",
    "        self.tdnn5 = TDNN(input_dim=512, output_dim=512, context_size=1, dilation=3,dropout_p=0.5)\n",
    "        #### Frame levelPooling\n",
    "        self.segment6 = nn.Linear(1024, 512)\n",
    "        self.segment7 = nn.Linear(512, 512)\n",
    "        self.output = nn.Linear(512, num_classes)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, inputs):\n",
    "        tdnn1_out = self.tdnn1(inputs)\n",
    "#         return tdnn1_out\n",
    "        tdnn2_out = self.tdnn2(tdnn1_out)\n",
    "        tdnn3_out = self.tdnn3(tdnn2_out)\n",
    "        tdnn4_out = self.tdnn4(tdnn3_out)\n",
    "        tdnn5_out = self.tdnn5(tdnn4_out)\n",
    "        ### Stat Pool\n",
    "        mean = torch.mean(tdnn5_out,1)\n",
    "        std = torch.std(tdnn5_out,1)\n",
    "        stat_pooling = torch.cat((mean,std),1)\n",
    "        segment6_out = self.segment6(stat_pooling)\n",
    "        x_vec = self.segment7(segment6_out)\n",
    "        predictions = self.output(x_vec)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feats = [4,100,24]\n",
    "input = torch.rand(input_feats)\n",
    "model = X_vector()\n",
    "out = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0430,  0.0629, -0.0228,  0.0610,  0.0439, -0.0419, -0.0279],\n",
       "        [ 0.0423,  0.0624, -0.0229,  0.0619,  0.0451, -0.0408, -0.0308],\n",
       "        [ 0.0442,  0.0637, -0.0208,  0.0601,  0.0420, -0.0420, -0.0305],\n",
       "        [ 0.0437,  0.0630, -0.0218,  0.0619,  0.0421, -0.0409, -0.0313]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "       layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader,epoch):\n",
    "    running_loss = 0.0\n",
    "    train_loss_list=[]\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad = True\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        labels = torch.argmax(labels,dim =1)\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_list.append(loss.item())\n",
    "        mean_loss = np.mean(np.asarray(train_loss_list))\n",
    "        if i%100==0:\n",
    "            print('Iteration - {} Epoch - {} Total training loss - {} '.format(i,epoch,mean_loss))\n",
    "            \n",
    "def validation(valid_dataloader,epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_list=[]\n",
    "        for i, data in enumerate(valid_dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            labels = torch.argmax(labels,dim =1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_list.append(loss.item())\n",
    "            if i%100==0:\n",
    "                print('Iteration - {} Epoch - {} Loss - {}'.format(i,epoch,np.mean(np.asarray(val_loss_list))))\n",
    "                \n",
    "        mean_loss = np.mean(np.asarray(val_loss_list))\n",
    "        print('Total validation loss {} after {} epochs'.format(mean_loss,epoch))\n",
    "        model_save_path = os.path.join( 'best_check_point_'+str(epoch)+'_'+str(mean_loss))\n",
    "        state_dict = {'model': model.state_dict(),'optimizer': optimizer.state_dict(),'epoch': epoch}\n",
    "        torch.save(state_dict, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 0 Epoch - 0 Total training loss - 1.932255744934082 \n",
      "Iteration - 100 Epoch - 0 Total training loss - 1.3132809970048394 \n",
      "Iteration - 200 Epoch - 0 Total training loss - 1.2890096272513345 \n",
      "Iteration - 300 Epoch - 0 Total training loss - 1.2668476909572302 \n",
      "Iteration - 400 Epoch - 0 Total training loss - 1.28971478802865 \n",
      "Iteration - 500 Epoch - 0 Total training loss - 1.26285388321428 \n",
      "Iteration - 600 Epoch - 0 Total training loss - 1.256680853001637 \n",
      "Iteration - 0 Epoch - 0 Loss - 1.2015089988708496\n",
      "Iteration - 100 Epoch - 0 Loss - 1.0766516071144898\n",
      "Iteration - 200 Epoch - 0 Loss - 1.0732556615599351\n",
      "Iteration - 300 Epoch - 0 Loss - 1.0746253565498365\n",
      "Iteration - 400 Epoch - 0 Loss - 1.0840016880683472\n",
      "Iteration - 500 Epoch - 0 Loss - 1.086331317346253\n",
      "Iteration - 600 Epoch - 0 Loss - 1.0843947761368236\n",
      "Total validation loss 1.0867940213723748 after 0 epochs\n",
      "Iteration - 0 Epoch - 1 Total training loss - 2.2937402725219727 \n",
      "Iteration - 100 Epoch - 1 Total training loss - 1.1011243586493011 \n",
      "Iteration - 200 Epoch - 1 Total training loss - 1.169448632654266 \n",
      "Iteration - 300 Epoch - 1 Total training loss - 1.1515794548382394 \n",
      "Iteration - 400 Epoch - 1 Total training loss - 1.1708005632917184 \n",
      "Iteration - 500 Epoch - 1 Total training loss - 1.1623415401774253 \n",
      "Iteration - 600 Epoch - 1 Total training loss - 1.1550232476729918 \n",
      "Iteration - 0 Epoch - 1 Loss - 1.0631916522979736\n",
      "Iteration - 100 Epoch - 1 Loss - 1.1334302611280196\n",
      "Iteration - 200 Epoch - 1 Loss - 1.1611304564855585\n",
      "Iteration - 300 Epoch - 1 Loss - 1.1735576668450998\n",
      "Iteration - 400 Epoch - 1 Loss - 1.1749303224972656\n",
      "Iteration - 500 Epoch - 1 Loss - 1.1776296124605838\n",
      "Iteration - 600 Epoch - 1 Loss - 1.1742213217668644\n",
      "Total validation loss 1.1741894714556929 after 1 epochs\n",
      "Iteration - 0 Epoch - 2 Total training loss - 0.5778006315231323 \n",
      "Iteration - 100 Epoch - 2 Total training loss - 1.0453527761567938 \n",
      "Iteration - 200 Epoch - 2 Total training loss - 1.0770480907081965 \n",
      "Iteration - 300 Epoch - 2 Total training loss - 1.090598712628862 \n",
      "Iteration - 400 Epoch - 2 Total training loss - 1.0941156817790576 \n",
      "Iteration - 500 Epoch - 2 Total training loss - 1.105268499451483 \n",
      "Iteration - 600 Epoch - 2 Total training loss - 1.1048921506833316 \n",
      "Iteration - 0 Epoch - 2 Loss - 1.4375101327896118\n",
      "Iteration - 100 Epoch - 2 Loss - 1.13161303619347\n",
      "Iteration - 200 Epoch - 2 Loss - 1.1178025997693266\n",
      "Iteration - 300 Epoch - 2 Loss - 1.1068815160233316\n",
      "Iteration - 400 Epoch - 2 Loss - 1.0914097796949067\n",
      "Iteration - 500 Epoch - 2 Loss - 1.089215523111606\n",
      "Iteration - 600 Epoch - 2 Loss - 1.0982957839866645\n",
      "Total validation loss 1.0915524078176377 after 2 epochs\n",
      "Iteration - 0 Epoch - 3 Total training loss - 0.7617074251174927 \n",
      "Iteration - 100 Epoch - 3 Total training loss - 1.1538987962326201 \n",
      "Iteration - 200 Epoch - 3 Total training loss - 1.1524609505981949 \n",
      "Iteration - 300 Epoch - 3 Total training loss - 1.1363778559373463 \n",
      "Iteration - 400 Epoch - 3 Total training loss - 1.1221866168063181 \n",
      "Iteration - 500 Epoch - 3 Total training loss - 1.1123885155735378 \n",
      "Iteration - 600 Epoch - 3 Total training loss - 1.0945191584440714 \n",
      "Iteration - 0 Epoch - 3 Loss - 1.139947533607483\n",
      "Iteration - 100 Epoch - 3 Loss - 1.1020363955804617\n",
      "Iteration - 200 Epoch - 3 Loss - 1.0947863194183332\n",
      "Iteration - 300 Epoch - 3 Loss - 1.1030340515497912\n",
      "Iteration - 400 Epoch - 3 Loss - 1.104177501730788\n",
      "Iteration - 500 Epoch - 3 Loss - 1.107914027875055\n",
      "Iteration - 600 Epoch - 3 Loss - 1.1181837766370837\n",
      "Total validation loss 1.116428358984211 after 3 epochs\n",
      "Iteration - 0 Epoch - 4 Total training loss - 0.8378793001174927 \n",
      "Iteration - 100 Epoch - 4 Total training loss - 1.065744932335202 \n",
      "Iteration - 200 Epoch - 4 Total training loss - 1.1194201017271226 \n",
      "Iteration - 300 Epoch - 4 Total training loss - 1.1100457440628562 \n",
      "Iteration - 400 Epoch - 4 Total training loss - 1.0928747785693391 \n",
      "Iteration - 500 Epoch - 4 Total training loss - 1.0765120342165648 \n",
      "Iteration - 600 Epoch - 4 Total training loss - 1.0714256261876736 \n",
      "Iteration - 0 Epoch - 4 Loss - 1.0034334659576416\n",
      "Iteration - 100 Epoch - 4 Loss - 0.932737006704406\n",
      "Iteration - 200 Epoch - 4 Loss - 0.9769600862590828\n",
      "Iteration - 300 Epoch - 4 Loss - 0.9717700117054175\n",
      "Iteration - 400 Epoch - 4 Loss - 0.9758830830789267\n",
      "Iteration - 500 Epoch - 4 Loss - 0.9825903144544232\n",
      "Iteration - 600 Epoch - 4 Loss - 0.9983344974613031\n",
      "Total validation loss 0.9988220760706348 after 4 epochs\n",
      "Iteration - 0 Epoch - 5 Total training loss - 1.7840443849563599 \n",
      "Iteration - 100 Epoch - 5 Total training loss - 1.0904277944476297 \n",
      "Iteration - 200 Epoch - 5 Total training loss - 1.0376166321374292 \n",
      "Iteration - 300 Epoch - 5 Total training loss - 1.1010749842785919 \n",
      "Iteration - 400 Epoch - 5 Total training loss - 1.0849742720140483 \n",
      "Iteration - 500 Epoch - 5 Total training loss - 1.0637781474360093 \n",
      "Iteration - 600 Epoch - 5 Total training loss - 1.0460058627789608 \n",
      "Iteration - 0 Epoch - 5 Loss - 0.9747180342674255\n",
      "Iteration - 100 Epoch - 5 Loss - 1.0109185682665003\n",
      "Iteration - 200 Epoch - 5 Loss - 1.023807149917925\n",
      "Iteration - 300 Epoch - 5 Loss - 1.0272834054952444\n",
      "Iteration - 400 Epoch - 5 Loss - 1.026158683132055\n",
      "Iteration - 500 Epoch - 5 Loss - 1.0274698121878438\n",
      "Iteration - 600 Epoch - 5 Loss - 1.028830987616704\n",
      "Total validation loss 1.02667003555229 after 5 epochs\n",
      "Iteration - 0 Epoch - 6 Total training loss - 0.906362771987915 \n",
      "Iteration - 100 Epoch - 6 Total training loss - 1.0413629997632292 \n",
      "Iteration - 200 Epoch - 6 Total training loss - 1.0496509382084234 \n",
      "Iteration - 300 Epoch - 6 Total training loss - 1.0523439193781825 \n",
      "Iteration - 400 Epoch - 6 Total training loss - 1.0406901615293247 \n",
      "Iteration - 500 Epoch - 6 Total training loss - 1.0470424471292905 \n",
      "Iteration - 600 Epoch - 6 Total training loss - 1.0530815244638385 \n",
      "Iteration - 0 Epoch - 6 Loss - 1.047781229019165\n",
      "Iteration - 100 Epoch - 6 Loss - 1.070315030836823\n",
      "Iteration - 200 Epoch - 6 Loss - 1.0928721168444524\n",
      "Iteration - 300 Epoch - 6 Loss - 1.1126923720503963\n",
      "Iteration - 400 Epoch - 6 Loss - 1.1029153863836703\n",
      "Iteration - 500 Epoch - 6 Loss - 1.101357568880755\n",
      "Iteration - 600 Epoch - 6 Loss - 1.0948270262775326\n",
      "Total validation loss 1.0849305888167993 after 6 epochs\n",
      "Iteration - 0 Epoch - 7 Total training loss - 0.7697468996047974 \n",
      "Iteration - 100 Epoch - 7 Total training loss - 0.995545366317919 \n",
      "Iteration - 200 Epoch - 7 Total training loss - 1.0205136313663787 \n",
      "Iteration - 300 Epoch - 7 Total training loss - 1.001763612923036 \n",
      "Iteration - 400 Epoch - 7 Total training loss - 0.9928474251338817 \n",
      "Iteration - 500 Epoch - 7 Total training loss - 1.0232705782407414 \n",
      "Iteration - 600 Epoch - 7 Total training loss - 1.0195446806260928 \n",
      "Iteration - 0 Epoch - 7 Loss - 1.5661230087280273\n",
      "Iteration - 100 Epoch - 7 Loss - 0.9147555133198747\n",
      "Iteration - 200 Epoch - 7 Loss - 0.9019395419910773\n",
      "Iteration - 300 Epoch - 7 Loss - 0.9130967068315741\n",
      "Iteration - 400 Epoch - 7 Loss - 0.9013786618771993\n",
      "Iteration - 500 Epoch - 7 Loss - 0.9078976727590827\n",
      "Iteration - 600 Epoch - 7 Loss - 0.909739840224063\n",
      "Total validation loss 0.9089895855737312 after 7 epochs\n",
      "Iteration - 0 Epoch - 8 Total training loss - 1.3809621334075928 \n",
      "Iteration - 100 Epoch - 8 Total training loss - 0.8624501541847049 \n",
      "Iteration - 200 Epoch - 8 Total training loss - 0.9201073755894728 \n",
      "Iteration - 300 Epoch - 8 Total training loss - 0.9484424068533701 \n",
      "Iteration - 400 Epoch - 8 Total training loss - 0.9507350731000044 \n",
      "Iteration - 500 Epoch - 8 Total training loss - 0.9864717957799782 \n",
      "Iteration - 600 Epoch - 8 Total training loss - 0.9922290024612589 \n",
      "Iteration - 0 Epoch - 8 Loss - 0.7385923862457275\n",
      "Iteration - 100 Epoch - 8 Loss - 0.8952464948196223\n",
      "Iteration - 200 Epoch - 8 Loss - 0.8822033730757177\n",
      "Iteration - 300 Epoch - 8 Loss - 0.8861194824756577\n",
      "Iteration - 400 Epoch - 8 Loss - 0.9017826819286084\n",
      "Iteration - 500 Epoch - 8 Loss - 0.8970557376593649\n",
      "Iteration - 600 Epoch - 8 Loss - 0.9046147795713285\n",
      "Total validation loss 0.9110424189536768 after 8 epochs\n",
      "Iteration - 0 Epoch - 9 Total training loss - 0.521129846572876 \n",
      "Iteration - 100 Epoch - 9 Total training loss - 0.9490048118894643 \n",
      "Iteration - 200 Epoch - 9 Total training loss - 0.9836346302644827 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 300 Epoch - 9 Total training loss - 0.9659724588137727 \n",
      "Iteration - 400 Epoch - 9 Total training loss - 0.9537861545725802 \n",
      "Iteration - 500 Epoch - 9 Total training loss - 0.9362265076004935 \n",
      "Iteration - 600 Epoch - 9 Total training loss - 0.9701674697415503 \n",
      "Iteration - 0 Epoch - 9 Loss - 1.0101689100265503\n",
      "Iteration - 100 Epoch - 9 Loss - 0.9896002311517696\n",
      "Iteration - 200 Epoch - 9 Loss - 0.976959256983515\n",
      "Iteration - 300 Epoch - 9 Loss - 0.9644247058321075\n",
      "Iteration - 400 Epoch - 9 Loss - 0.9574731065522406\n",
      "Iteration - 500 Epoch - 9 Loss - 0.9622777105864888\n",
      "Iteration - 600 Epoch - 9 Loss - 0.9564418919669609\n",
      "Total validation loss 0.9551710378163492 after 9 epochs\n",
      "Iteration - 0 Epoch - 10 Total training loss - 0.7006010413169861 \n",
      "Iteration - 100 Epoch - 10 Total training loss - 0.9483041629047677 \n",
      "Iteration - 200 Epoch - 10 Total training loss - 0.9101804855895873 \n",
      "Iteration - 300 Epoch - 10 Total training loss - 0.901070951425356 \n",
      "Iteration - 400 Epoch - 10 Total training loss - 0.9272334597438128 \n",
      "Iteration - 500 Epoch - 10 Total training loss - 0.9513923654336475 \n",
      "Iteration - 600 Epoch - 10 Total training loss - 0.9659824467197482 \n",
      "Iteration - 0 Epoch - 10 Loss - 0.737095832824707\n",
      "Iteration - 100 Epoch - 10 Loss - 0.9418743128823762\n",
      "Iteration - 200 Epoch - 10 Loss - 0.9716847600109542\n",
      "Iteration - 300 Epoch - 10 Loss - 0.9605821899549906\n",
      "Iteration - 400 Epoch - 10 Loss - 0.9381319463364501\n",
      "Iteration - 500 Epoch - 10 Loss - 0.9357283539965957\n",
      "Iteration - 600 Epoch - 10 Loss - 0.9215803517403698\n",
      "Total validation loss 0.9180698331932348 after 10 epochs\n",
      "Iteration - 0 Epoch - 11 Total training loss - 1.216096043586731 \n",
      "Iteration - 100 Epoch - 11 Total training loss - 0.8321320288030818 \n",
      "Iteration - 200 Epoch - 11 Total training loss - 0.8606552674217307 \n",
      "Iteration - 300 Epoch - 11 Total training loss - 0.8864254008025624 \n",
      "Iteration - 400 Epoch - 11 Total training loss - 0.885773241640698 \n",
      "Iteration - 500 Epoch - 11 Total training loss - 0.8917310662240444 \n",
      "Iteration - 600 Epoch - 11 Total training loss - 0.9012207384229004 \n",
      "Iteration - 0 Epoch - 11 Loss - 1.0393669605255127\n",
      "Iteration - 100 Epoch - 11 Loss - 0.8992145344762519\n",
      "Iteration - 200 Epoch - 11 Loss - 0.9328190331435322\n",
      "Iteration - 300 Epoch - 11 Loss - 0.9267848508302555\n",
      "Iteration - 400 Epoch - 11 Loss - 0.9414519356224602\n",
      "Iteration - 500 Epoch - 11 Loss - 0.92709182842287\n",
      "Iteration - 600 Epoch - 11 Loss - 0.9174815680143639\n",
      "Total validation loss 0.9188179032237334 after 11 epochs\n",
      "Iteration - 0 Epoch - 12 Total training loss - 0.800360918045044 \n",
      "Iteration - 100 Epoch - 12 Total training loss - 0.888341027468738 \n",
      "Iteration - 200 Epoch - 12 Total training loss - 0.9181523340110161 \n",
      "Iteration - 300 Epoch - 12 Total training loss - 0.912573284831554 \n",
      "Iteration - 400 Epoch - 12 Total training loss - 0.8984049324903107 \n",
      "Iteration - 500 Epoch - 12 Total training loss - 0.9156269220088771 \n",
      "Iteration - 600 Epoch - 12 Total training loss - 0.9255617951816212 \n",
      "Iteration - 0 Epoch - 12 Loss - 1.1865873336791992\n",
      "Iteration - 100 Epoch - 12 Loss - 0.9061502618069696\n",
      "Iteration - 200 Epoch - 12 Loss - 0.9024169175927319\n",
      "Iteration - 300 Epoch - 12 Loss - 0.9156824456298867\n",
      "Iteration - 400 Epoch - 12 Loss - 0.9257574517307733\n",
      "Iteration - 500 Epoch - 12 Loss - 0.9425834060726528\n",
      "Iteration - 600 Epoch - 12 Loss - 0.92856916683585\n",
      "Total validation loss 0.925880125092518 after 12 epochs\n",
      "Iteration - 0 Epoch - 13 Total training loss - 0.7114898562431335 \n",
      "Iteration - 100 Epoch - 13 Total training loss - 1.027315443916486 \n",
      "Iteration - 200 Epoch - 13 Total training loss - 0.9849615007157053 \n",
      "Iteration - 300 Epoch - 13 Total training loss - 0.9471667961796059 \n",
      "Iteration - 400 Epoch - 13 Total training loss - 0.9371513113509539 \n",
      "Iteration - 500 Epoch - 13 Total training loss - 0.9380470807041356 \n",
      "Iteration - 600 Epoch - 13 Total training loss - 0.9292773815389481 \n",
      "Iteration - 0 Epoch - 13 Loss - 0.7299056053161621\n",
      "Iteration - 100 Epoch - 13 Loss - 0.8772915790576746\n",
      "Iteration - 200 Epoch - 13 Loss - 0.8931483977766179\n",
      "Iteration - 300 Epoch - 13 Loss - 0.890442220624103\n",
      "Iteration - 400 Epoch - 13 Loss - 0.899279972227137\n",
      "Iteration - 500 Epoch - 13 Loss - 0.8931798851359152\n",
      "Iteration - 600 Epoch - 13 Loss - 0.8941296728052037\n",
      "Total validation loss 0.8916616735351485 after 13 epochs\n",
      "Iteration - 0 Epoch - 14 Total training loss - 1.361981749534607 \n",
      "Iteration - 100 Epoch - 14 Total training loss - 0.8700991656658894 \n",
      "Iteration - 200 Epoch - 14 Total training loss - 0.8221335584524815 \n",
      "Iteration - 300 Epoch - 14 Total training loss - 0.8565156179220351 \n",
      "Iteration - 400 Epoch - 14 Total training loss - 0.8676892300484762 \n",
      "Iteration - 500 Epoch - 14 Total training loss - 0.8604347090452478 \n",
      "Iteration - 600 Epoch - 14 Total training loss - 0.8832190634547957 \n",
      "Iteration - 0 Epoch - 14 Loss - 1.3842356204986572\n",
      "Iteration - 100 Epoch - 14 Loss - 0.9580265035723695\n",
      "Iteration - 200 Epoch - 14 Loss - 0.9626088618342556\n",
      "Iteration - 300 Epoch - 14 Loss - 0.9751801869995966\n",
      "Iteration - 400 Epoch - 14 Loss - 0.985036376185548\n",
      "Iteration - 500 Epoch - 14 Loss - 0.9890130011740321\n",
      "Iteration - 600 Epoch - 14 Loss - 0.9753356088268975\n",
      "Total validation loss 0.9743060567458712 after 14 epochs\n",
      "Iteration - 0 Epoch - 15 Total training loss - 1.1779365539550781 \n",
      "Iteration - 100 Epoch - 15 Total training loss - 0.7696897639883774 \n",
      "Iteration - 200 Epoch - 15 Total training loss - 0.8447124314406396 \n",
      "Iteration - 300 Epoch - 15 Total training loss - 0.8521647126726385 \n",
      "Iteration - 400 Epoch - 15 Total training loss - 0.9083892411652237 \n",
      "Iteration - 500 Epoch - 15 Total training loss - 0.9215349905032985 \n",
      "Iteration - 600 Epoch - 15 Total training loss - 0.9103840026592281 \n",
      "Iteration - 0 Epoch - 15 Loss - 1.3701720237731934\n",
      "Iteration - 100 Epoch - 15 Loss - 0.9035490358820056\n",
      "Iteration - 200 Epoch - 15 Loss - 0.8978099448244963\n",
      "Iteration - 300 Epoch - 15 Loss - 0.8714288018916136\n",
      "Iteration - 400 Epoch - 15 Loss - 0.8858542582972389\n",
      "Iteration - 500 Epoch - 15 Loss - 0.8905138854762751\n",
      "Iteration - 600 Epoch - 15 Loss - 0.8975169958791598\n",
      "Total validation loss 0.8933955705738811 after 15 epochs\n",
      "Iteration - 0 Epoch - 16 Total training loss - 0.8371995091438293 \n",
      "Iteration - 100 Epoch - 16 Total training loss - 0.8578438651148635 \n",
      "Iteration - 200 Epoch - 16 Total training loss - 0.9018574980694559 \n",
      "Iteration - 300 Epoch - 16 Total training loss - 0.8828223170557885 \n",
      "Iteration - 400 Epoch - 16 Total training loss - 0.9107961122810543 \n",
      "Iteration - 500 Epoch - 16 Total training loss - 0.8796825322123643 \n",
      "Iteration - 600 Epoch - 16 Total training loss - 0.8856732138957885 \n",
      "Iteration - 0 Epoch - 16 Loss - 0.8908408284187317\n",
      "Iteration - 100 Epoch - 16 Loss - 0.8939702146419204\n",
      "Iteration - 200 Epoch - 16 Loss - 0.8961745904601035\n",
      "Iteration - 300 Epoch - 16 Loss - 0.9110216216688536\n",
      "Iteration - 400 Epoch - 16 Loss - 0.9060944362991766\n",
      "Iteration - 500 Epoch - 16 Loss - 0.9065627551185871\n",
      "Iteration - 600 Epoch - 16 Loss - 0.9099691039115537\n",
      "Total validation loss 0.9055565674661865 after 16 epochs\n",
      "Iteration - 0 Epoch - 17 Total training loss - 0.6941127777099609 \n",
      "Iteration - 100 Epoch - 17 Total training loss - 0.8587136603154168 \n",
      "Iteration - 200 Epoch - 17 Total training loss - 0.9404990044584618 \n",
      "Iteration - 300 Epoch - 17 Total training loss - 0.8987581869989535 \n",
      "Iteration - 400 Epoch - 17 Total training loss - 0.8968443597418412 \n",
      "Iteration - 500 Epoch - 17 Total training loss - 0.9139128799091081 \n",
      "Iteration - 600 Epoch - 17 Total training loss - 0.9259014578106908 \n",
      "Iteration - 0 Epoch - 17 Loss - 0.39153242111206055\n",
      "Iteration - 100 Epoch - 17 Loss - 0.8013026975464113\n",
      "Iteration - 200 Epoch - 17 Loss - 0.821937984717426\n",
      "Iteration - 300 Epoch - 17 Loss - 0.8121052025461514\n",
      "Iteration - 400 Epoch - 17 Loss - 0.8200927021869103\n",
      "Iteration - 500 Epoch - 17 Loss - 0.818666615291032\n",
      "Iteration - 600 Epoch - 17 Loss - 0.8185951450898524\n",
      "Total validation loss 0.817686213140792 after 17 epochs\n",
      "Iteration - 0 Epoch - 18 Total training loss - 0.01934763416647911 \n",
      "Iteration - 100 Epoch - 18 Total training loss - 1.0330891307111423 \n",
      "Iteration - 200 Epoch - 18 Total training loss - 0.9462834814776532 \n",
      "Iteration - 300 Epoch - 18 Total training loss - 1.4878854635187062 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 400 Epoch - 18 Total training loss - 1.3977029264075427 \n",
      "Iteration - 500 Epoch - 18 Total training loss - 1.2848768669353452 \n",
      "Iteration - 600 Epoch - 18 Total training loss - 1.1970177485627709 \n",
      "Iteration - 0 Epoch - 18 Loss - 1.3501555919647217\n",
      "Iteration - 100 Epoch - 18 Loss - 0.8764643058328345\n",
      "Iteration - 200 Epoch - 18 Loss - 0.8728582899072277\n",
      "Iteration - 300 Epoch - 18 Loss - 0.8648361109716948\n",
      "Iteration - 400 Epoch - 18 Loss - 0.8733248997879147\n",
      "Iteration - 500 Epoch - 18 Loss - 0.8844458882263797\n",
      "Iteration - 600 Epoch - 18 Loss - 0.8818372419590561\n",
      "Total validation loss 0.881375299896875 after 18 epochs\n",
      "Iteration - 0 Epoch - 19 Total training loss - 0.9488004446029663 \n",
      "Iteration - 100 Epoch - 19 Total training loss - 0.8663161632669444 \n",
      "Iteration - 200 Epoch - 19 Total training loss - 0.9176895846913925 \n",
      "Iteration - 300 Epoch - 19 Total training loss - 0.9128416891478968 \n",
      "Iteration - 400 Epoch - 19 Total training loss - 0.9046306170707805 \n",
      "Iteration - 500 Epoch - 19 Total training loss - 0.8923732723140096 \n",
      "Iteration - 600 Epoch - 19 Total training loss - 0.8878016488982238 \n",
      "Iteration - 0 Epoch - 19 Loss - 0.7221680879592896\n",
      "Iteration - 100 Epoch - 19 Loss - 1.1075737564870627\n",
      "Iteration - 200 Epoch - 19 Loss - 1.1061926120845833\n",
      "Iteration - 300 Epoch - 19 Loss - 1.1151221922267711\n",
      "Iteration - 400 Epoch - 19 Loss - 1.114674788014847\n",
      "Iteration - 500 Epoch - 19 Loss - 1.1157955328623455\n",
      "Iteration - 600 Epoch - 19 Loss - 1.1206639792875521\n",
      "Total validation loss 1.1188418471160992 after 19 epochs\n",
      "Iteration - 0 Epoch - 20 Total training loss - 1.4312461614608765 \n",
      "Iteration - 100 Epoch - 20 Total training loss - 0.9050647041732722 \n",
      "Iteration - 200 Epoch - 20 Total training loss - 0.8471161053117494 \n",
      "Iteration - 300 Epoch - 20 Total training loss - 0.8882905736865717 \n",
      "Iteration - 400 Epoch - 20 Total training loss - 0.9902888189343981 \n",
      "Iteration - 500 Epoch - 20 Total training loss - 0.9590352246558252 \n",
      "Iteration - 600 Epoch - 20 Total training loss - 0.9427751442192808 \n",
      "Iteration - 0 Epoch - 20 Loss - 0.6668903827667236\n",
      "Iteration - 100 Epoch - 20 Loss - 0.9349686844219075\n",
      "Iteration - 200 Epoch - 20 Loss - 0.9530795490118995\n",
      "Iteration - 300 Epoch - 20 Loss - 0.964814000599012\n",
      "Iteration - 400 Epoch - 20 Loss - 0.9414375426243071\n",
      "Iteration - 500 Epoch - 20 Loss - 0.9461145456620081\n",
      "Iteration - 600 Epoch - 20 Loss - 0.9507718601212922\n",
      "Total validation loss 0.9478903316119884 after 20 epochs\n",
      "Iteration - 0 Epoch - 21 Total training loss - 1.1712594032287598 \n",
      "Iteration - 100 Epoch - 21 Total training loss - 0.7596425457608582 \n",
      "Iteration - 200 Epoch - 21 Total training loss - 0.8178183934118469 \n",
      "Iteration - 300 Epoch - 21 Total training loss - 0.8841283703130048 \n",
      "Iteration - 400 Epoch - 21 Total training loss - 0.8642916810603233 \n",
      "Iteration - 500 Epoch - 21 Total training loss - 0.8664627318305542 \n",
      "Iteration - 600 Epoch - 21 Total training loss - 0.8715779325070737 \n",
      "Iteration - 0 Epoch - 21 Loss - 0.7962827086448669\n",
      "Iteration - 100 Epoch - 21 Loss - 0.921719040168394\n",
      "Iteration - 200 Epoch - 21 Loss - 0.9151161278213434\n",
      "Iteration - 300 Epoch - 21 Loss - 0.8916520500598952\n",
      "Iteration - 400 Epoch - 21 Loss - 0.8826751071690324\n",
      "Iteration - 500 Epoch - 21 Loss - 0.8898540864031234\n",
      "Iteration - 600 Epoch - 21 Loss - 0.9061484138501464\n",
      "Total validation loss 0.8973687030261773 after 21 epochs\n",
      "Iteration - 0 Epoch - 22 Total training loss - 0.07246160507202148 \n",
      "Iteration - 100 Epoch - 22 Total training loss - 0.9027532920388892 \n",
      "Iteration - 200 Epoch - 22 Total training loss - 0.9045130473223344 \n",
      "Iteration - 300 Epoch - 22 Total training loss - 0.9166634052224532 \n",
      "Iteration - 400 Epoch - 22 Total training loss - 0.9077677408309782 \n",
      "Iteration - 500 Epoch - 22 Total training loss - 0.9039996615659233 \n",
      "Iteration - 600 Epoch - 22 Total training loss - 0.9099134641814068 \n",
      "Iteration - 0 Epoch - 22 Loss - 1.2014527320861816\n",
      "Iteration - 100 Epoch - 22 Loss - 0.8652736245110484\n",
      "Iteration - 200 Epoch - 22 Loss - 0.884145761149662\n",
      "Iteration - 300 Epoch - 22 Loss - 0.9146608276063719\n",
      "Iteration - 400 Epoch - 22 Loss - 0.8958939297491698\n",
      "Iteration - 500 Epoch - 22 Loss - 0.9038359921931209\n",
      "Iteration - 600 Epoch - 22 Loss - 0.9031311293874872\n",
      "Total validation loss 0.9028887759107179 after 22 epochs\n",
      "Iteration - 0 Epoch - 23 Total training loss - 0.7948232293128967 \n",
      "Iteration - 100 Epoch - 23 Total training loss - 0.8776696204844088 \n",
      "Iteration - 200 Epoch - 23 Total training loss - 0.895287964437435 \n",
      "Iteration - 300 Epoch - 23 Total training loss - 0.8842616834835555 \n",
      "Iteration - 400 Epoch - 23 Total training loss - 0.8794628585765122 \n",
      "Iteration - 500 Epoch - 23 Total training loss - 0.8836134598127329 \n",
      "Iteration - 600 Epoch - 23 Total training loss - 0.882295872830455 \n",
      "Iteration - 0 Epoch - 23 Loss - 0.6790464520454407\n",
      "Iteration - 100 Epoch - 23 Loss - 0.8882433896017546\n",
      "Iteration - 200 Epoch - 23 Loss - 0.8983281173219728\n",
      "Iteration - 300 Epoch - 23 Loss - 0.9260723416591403\n",
      "Iteration - 400 Epoch - 23 Loss - 0.9156056181748312\n",
      "Iteration - 500 Epoch - 23 Loss - 0.9229558027552035\n",
      "Iteration - 600 Epoch - 23 Loss - 0.9211153629020525\n",
      "Total validation loss 0.9218581482360428 after 23 epochs\n",
      "Iteration - 0 Epoch - 24 Total training loss - 1.4085665941238403 \n",
      "Iteration - 100 Epoch - 24 Total training loss - 1.0225853678668932 \n",
      "Iteration - 200 Epoch - 24 Total training loss - 0.9004914541473955 \n",
      "Iteration - 300 Epoch - 24 Total training loss - 0.9086721097358945 \n",
      "Iteration - 400 Epoch - 24 Total training loss - 0.8742940810783661 \n",
      "Iteration - 500 Epoch - 24 Total training loss - 0.8797666949789601 \n",
      "Iteration - 600 Epoch - 24 Total training loss - 0.8655156575045189 \n",
      "Iteration - 0 Epoch - 24 Loss - 0.7635825872421265\n",
      "Iteration - 100 Epoch - 24 Loss - 1.0339162284784977\n",
      "Iteration - 200 Epoch - 24 Loss - 1.0338942292910904\n",
      "Iteration - 300 Epoch - 24 Loss - 1.0265889215310944\n",
      "Iteration - 400 Epoch - 24 Loss - 1.018708535412006\n",
      "Iteration - 500 Epoch - 24 Loss - 1.0080218464433552\n",
      "Iteration - 600 Epoch - 24 Loss - 1.0071391134512009\n",
      "Total validation loss 1.004544051584864 after 24 epochs\n",
      "Iteration - 0 Epoch - 25 Total training loss - 0.8693932890892029 \n",
      "Iteration - 100 Epoch - 25 Total training loss - 0.9682157500693114 \n",
      "Iteration - 200 Epoch - 25 Total training loss - 0.9380928629369878 \n",
      "Iteration - 300 Epoch - 25 Total training loss - 0.932091074652252 \n",
      "Iteration - 400 Epoch - 25 Total training loss - 0.9315623848366916 \n",
      "Iteration - 500 Epoch - 25 Total training loss - 0.9327707485148977 \n",
      "Iteration - 600 Epoch - 25 Total training loss - 0.9615645137369112 \n",
      "Iteration - 0 Epoch - 25 Loss - 0.9822821617126465\n",
      "Iteration - 100 Epoch - 25 Loss - 1.0636422746252305\n",
      "Iteration - 200 Epoch - 25 Loss - 1.0257067662566455\n",
      "Iteration - 300 Epoch - 25 Loss - 1.0374086787336292\n",
      "Iteration - 400 Epoch - 25 Loss - 1.0450211810946761\n",
      "Iteration - 500 Epoch - 25 Loss - 1.046115162070402\n",
      "Iteration - 600 Epoch - 25 Loss - 1.0452259771042378\n",
      "Total validation loss 1.04494837556023 after 25 epochs\n",
      "Iteration - 0 Epoch - 26 Total training loss - 2.586259365081787 \n",
      "Iteration - 100 Epoch - 26 Total training loss - 0.8877054581990337 \n",
      "Iteration - 200 Epoch - 26 Total training loss - 0.8601219732342149 \n",
      "Iteration - 300 Epoch - 26 Total training loss - 0.8947599176379335 \n",
      "Iteration - 400 Epoch - 26 Total training loss - 0.9616357112428792 \n",
      "Iteration - 500 Epoch - 26 Total training loss - 0.9481634103624288 \n",
      "Iteration - 600 Epoch - 26 Total training loss - 0.9662895851711217 \n",
      "Iteration - 0 Epoch - 26 Loss - 0.9064416885375977\n",
      "Iteration - 100 Epoch - 26 Loss - 0.9086893793970051\n",
      "Iteration - 200 Epoch - 26 Loss - 0.9153001040071991\n",
      "Iteration - 300 Epoch - 26 Loss - 0.9000705982561523\n",
      "Iteration - 400 Epoch - 26 Loss - 0.9033229740480533\n",
      "Iteration - 500 Epoch - 26 Loss - 0.9060832137714128\n",
      "Iteration - 600 Epoch - 26 Loss - 0.9032883160622068\n",
      "Total validation loss 0.9008830590392987 after 26 epochs\n",
      "Iteration - 0 Epoch - 27 Total training loss - 0.7349590063095093 \n",
      "Iteration - 100 Epoch - 27 Total training loss - 0.9647611451916175 \n",
      "Iteration - 200 Epoch - 27 Total training loss - 0.9104800131458636 \n",
      "Iteration - 300 Epoch - 27 Total training loss - 1.4325242518357224 \n",
      "Iteration - 400 Epoch - 27 Total training loss - 1.3054252582446304 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 500 Epoch - 27 Total training loss - 1.2958721914899094 \n",
      "Iteration - 600 Epoch - 27 Total training loss - 1.2373272276063334 \n",
      "Iteration - 0 Epoch - 27 Loss - 1.1655679941177368\n",
      "Iteration - 100 Epoch - 27 Loss - 0.911109330335466\n",
      "Iteration - 200 Epoch - 27 Loss - 0.9623365947485563\n",
      "Iteration - 300 Epoch - 27 Loss - 0.9516985027546503\n",
      "Iteration - 400 Epoch - 27 Loss - 0.9426370448713886\n",
      "Iteration - 500 Epoch - 27 Loss - 0.9455263527359316\n",
      "Iteration - 600 Epoch - 27 Loss - 0.9471932181626707\n",
      "Total validation loss 0.9471208674317979 after 27 epochs\n",
      "Iteration - 0 Epoch - 28 Total training loss - 0.1323520541191101 \n",
      "Iteration - 100 Epoch - 28 Total training loss - 0.7890179075190041 \n",
      "Iteration - 200 Epoch - 28 Total training loss - 0.8120794391481957 \n",
      "Iteration - 300 Epoch - 28 Total training loss - 0.8860519419204168 \n",
      "Iteration - 400 Epoch - 28 Total training loss - 0.90216447646887 \n",
      "Iteration - 500 Epoch - 28 Total training loss - 0.8727242993947235 \n",
      "Iteration - 600 Epoch - 28 Total training loss - 0.8948988037925956 \n",
      "Iteration - 0 Epoch - 28 Loss - 0.6092569828033447\n",
      "Iteration - 100 Epoch - 28 Loss - 0.9488699318158744\n",
      "Iteration - 200 Epoch - 28 Loss - 0.9773127194957354\n",
      "Iteration - 300 Epoch - 28 Loss - 0.9820461077151504\n",
      "Iteration - 400 Epoch - 28 Loss - 0.9779356015739298\n",
      "Iteration - 500 Epoch - 28 Loss - 0.980334251464722\n",
      "Iteration - 600 Epoch - 28 Loss - 0.9786092539793639\n",
      "Total validation loss 0.9795797811906026 after 28 epochs\n",
      "Iteration - 0 Epoch - 29 Total training loss - 1.2868139743804932 \n",
      "Iteration - 100 Epoch - 29 Total training loss - 0.7721044350747425 \n",
      "Iteration - 200 Epoch - 29 Total training loss - 0.8472039862582228 \n",
      "Iteration - 300 Epoch - 29 Total training loss - 0.8366439403910279 \n",
      "Iteration - 400 Epoch - 29 Total training loss - 0.8970383538411593 \n",
      "Iteration - 500 Epoch - 29 Total training loss - 0.8961193887615321 \n",
      "Iteration - 600 Epoch - 29 Total training loss - 0.8806716449028329 \n",
      "Iteration - 0 Epoch - 29 Loss - 0.8645356893539429\n",
      "Iteration - 100 Epoch - 29 Loss - 0.9734524056167886\n",
      "Iteration - 200 Epoch - 29 Loss - 0.9659481076013983\n",
      "Iteration - 300 Epoch - 29 Loss - 0.9490601918923103\n",
      "Iteration - 400 Epoch - 29 Loss - 0.9425744900196568\n",
      "Iteration - 500 Epoch - 29 Loss - 0.9395932136568242\n",
      "Iteration - 600 Epoch - 29 Loss - 0.950188836284267\n",
      "Total validation loss 0.9500167041841475 after 29 epochs\n",
      "Iteration - 0 Epoch - 30 Total training loss - 0.6760908365249634 \n",
      "Iteration - 100 Epoch - 30 Total training loss - 1.455616141617584 \n",
      "Iteration - 200 Epoch - 30 Total training loss - 1.1169140250184597 \n",
      "Iteration - 300 Epoch - 30 Total training loss - 1.0332277729729054 \n",
      "Iteration - 400 Epoch - 30 Total training loss - 0.9925712234281965 \n",
      "Iteration - 500 Epoch - 30 Total training loss - 0.9744011260364149 \n",
      "Iteration - 600 Epoch - 30 Total training loss - 0.973664915296652 \n",
      "Iteration - 0 Epoch - 30 Loss - 1.1009891033172607\n",
      "Iteration - 100 Epoch - 30 Loss - 0.9812748954806587\n",
      "Iteration - 200 Epoch - 30 Loss - 0.9609320733269945\n",
      "Iteration - 300 Epoch - 30 Loss - 0.9343273916291042\n",
      "Iteration - 400 Epoch - 30 Loss - 0.9412484081215841\n",
      "Iteration - 500 Epoch - 30 Loss - 0.9373794530932894\n",
      "Iteration - 600 Epoch - 30 Loss - 0.935449531924804\n",
      "Total validation loss 0.9377981085941693 after 30 epochs\n",
      "Iteration - 0 Epoch - 31 Total training loss - 3.256650447845459 \n",
      "Iteration - 100 Epoch - 31 Total training loss - 0.870984418548201 \n",
      "Iteration - 200 Epoch - 31 Total training loss - 0.855625031298184 \n",
      "Iteration - 300 Epoch - 31 Total training loss - 0.8299763193393895 \n",
      "Iteration - 400 Epoch - 31 Total training loss - 0.8491797187880933 \n",
      "Iteration - 500 Epoch - 31 Total training loss - 0.8768415553147465 \n",
      "Iteration - 600 Epoch - 31 Total training loss - 0.8754398938452659 \n",
      "Iteration - 0 Epoch - 31 Loss - 1.1831262111663818\n",
      "Iteration - 100 Epoch - 31 Loss - 1.0181109062810936\n",
      "Iteration - 200 Epoch - 31 Loss - 0.9812836253376149\n",
      "Iteration - 300 Epoch - 31 Loss - 0.9699479277644839\n",
      "Iteration - 400 Epoch - 31 Loss - 0.9641096324769042\n",
      "Iteration - 500 Epoch - 31 Loss - 0.9607067609023667\n",
      "Iteration - 600 Epoch - 31 Loss - 0.9554175900639392\n",
      "Total validation loss 0.9606565272599731 after 31 epochs\n",
      "Iteration - 0 Epoch - 32 Total training loss - 0.5462608337402344 \n",
      "Iteration - 100 Epoch - 32 Total training loss - 0.978721714034529 \n",
      "Iteration - 200 Epoch - 32 Total training loss - 0.9699099500482874 \n",
      "Iteration - 300 Epoch - 32 Total training loss - 0.9340358150164145 \n",
      "Iteration - 400 Epoch - 32 Total training loss - 0.9390744727281702 \n",
      "Iteration - 500 Epoch - 32 Total training loss - 0.9359344999031015 \n",
      "Iteration - 600 Epoch - 32 Total training loss - 1.04529652261563 \n",
      "Iteration - 0 Epoch - 32 Loss - 1.0451209545135498\n",
      "Iteration - 100 Epoch - 32 Loss - 1.0412423896907579\n",
      "Iteration - 200 Epoch - 32 Loss - 1.0552369139680815\n",
      "Iteration - 300 Epoch - 32 Loss - 1.044256185772411\n",
      "Iteration - 400 Epoch - 32 Loss - 1.047943465430243\n",
      "Iteration - 500 Epoch - 32 Loss - 1.0624259569330843\n",
      "Iteration - 600 Epoch - 32 Loss - 1.0646771977279033\n",
      "Total validation loss 1.061424943247646 after 32 epochs\n",
      "Iteration - 0 Epoch - 33 Total training loss - 1.0581512451171875 \n",
      "Iteration - 100 Epoch - 33 Total training loss - 0.8861613134936531 \n",
      "Iteration - 200 Epoch - 33 Total training loss - 0.9515490849683089 \n",
      "Iteration - 300 Epoch - 33 Total training loss - 0.9195927918340974 \n",
      "Iteration - 400 Epoch - 33 Total training loss - 0.9121046782145509 \n",
      "Iteration - 500 Epoch - 33 Total training loss - 0.9193791867719429 \n",
      "Iteration - 600 Epoch - 33 Total training loss - 0.9372642199941553 \n",
      "Iteration - 0 Epoch - 33 Loss - 0.5411181449890137\n",
      "Iteration - 100 Epoch - 33 Loss - 1.096956102269711\n",
      "Iteration - 200 Epoch - 33 Loss - 1.0942709936727932\n",
      "Iteration - 300 Epoch - 33 Loss - 1.095266464837762\n",
      "Iteration - 400 Epoch - 33 Loss - 1.0952949363394568\n",
      "Iteration - 500 Epoch - 33 Loss - 1.0828734677351877\n",
      "Iteration - 600 Epoch - 33 Loss - 1.0854205940408437\n",
      "Total validation loss 1.0852950702231705 after 33 epochs\n",
      "Iteration - 0 Epoch - 34 Total training loss - 0.5562039613723755 \n",
      "Iteration - 100 Epoch - 34 Total training loss - 1.017299561438584 \n",
      "Iteration - 200 Epoch - 34 Total training loss - 0.9185172855197823 \n",
      "Iteration - 300 Epoch - 34 Total training loss - 0.87826647712641 \n",
      "Iteration - 400 Epoch - 34 Total training loss - 0.9245669880547756 \n",
      "Iteration - 500 Epoch - 34 Total training loss - 0.9207434878158737 \n",
      "Iteration - 600 Epoch - 34 Total training loss - 0.9301474502496305 \n",
      "Iteration - 0 Epoch - 34 Loss - 1.060529351234436\n",
      "Iteration - 100 Epoch - 34 Loss - 0.9648099689200373\n",
      "Iteration - 200 Epoch - 34 Loss - 0.9456216077602918\n",
      "Iteration - 300 Epoch - 34 Loss - 0.9564086935448884\n",
      "Iteration - 400 Epoch - 34 Loss - 0.9477665088792097\n",
      "Iteration - 500 Epoch - 34 Loss - 0.9406574018998536\n",
      "Iteration - 600 Epoch - 34 Loss - 0.9306873949415077\n",
      "Total validation loss 0.9259405428696549 after 34 epochs\n",
      "Iteration - 0 Epoch - 35 Total training loss - 0.628002405166626 \n",
      "Iteration - 100 Epoch - 35 Total training loss - 0.8310923717252099 \n",
      "Iteration - 200 Epoch - 35 Total training loss - 0.8234910686843245 \n",
      "Iteration - 300 Epoch - 35 Total training loss - 0.9661234791119477 \n",
      "Iteration - 400 Epoch - 35 Total training loss - 0.9797398940080305 \n",
      "Iteration - 500 Epoch - 35 Total training loss - 0.9576690350834628 \n",
      "Iteration - 600 Epoch - 35 Total training loss - 0.9451552921728488 \n",
      "Iteration - 0 Epoch - 35 Loss - 1.5042123794555664\n",
      "Iteration - 100 Epoch - 35 Loss - 1.2256964254054692\n",
      "Iteration - 200 Epoch - 35 Loss - 1.2300565149727745\n",
      "Iteration - 300 Epoch - 35 Loss - 1.2160966300043552\n",
      "Iteration - 400 Epoch - 35 Loss - 1.1929469407440867\n",
      "Iteration - 500 Epoch - 35 Loss - 1.199543026757395\n",
      "Iteration - 600 Epoch - 35 Loss - 1.2109097578623231\n",
      "Total validation loss 1.2103361993922646 after 35 epochs\n",
      "Iteration - 0 Epoch - 36 Total training loss - 1.6951687335968018 \n",
      "Iteration - 100 Epoch - 36 Total training loss - 1.021673060526569 \n",
      "Iteration - 200 Epoch - 36 Total training loss - 2.354759309916612 \n",
      "Iteration - 300 Epoch - 36 Total training loss - 1.8633634905008423 \n",
      "Iteration - 400 Epoch - 36 Total training loss - 1.6409497204440595 \n",
      "Iteration - 500 Epoch - 36 Total training loss - 1.5261645651920257 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 600 Epoch - 36 Total training loss - 1.481706055567201 \n",
      "Iteration - 0 Epoch - 36 Loss - 0.8200722932815552\n",
      "Iteration - 100 Epoch - 36 Loss - 1.0724506667344877\n",
      "Iteration - 200 Epoch - 36 Loss - 1.0760306033922071\n",
      "Iteration - 300 Epoch - 36 Loss - 1.0625822692614457\n",
      "Iteration - 400 Epoch - 36 Loss - 1.0837521129415517\n",
      "Iteration - 500 Epoch - 36 Loss - 1.0710165798366664\n",
      "Iteration - 600 Epoch - 36 Loss - 1.0681513630461177\n",
      "Total validation loss 1.0701742943795256 after 36 epochs\n",
      "Iteration - 0 Epoch - 37 Total training loss - 0.8585156202316284 \n",
      "Iteration - 100 Epoch - 37 Total training loss - 1.0162152769320671 \n",
      "Iteration - 200 Epoch - 37 Total training loss - 0.9221201275049046 \n",
      "Iteration - 300 Epoch - 37 Total training loss - 0.9186782637546297 \n",
      "Iteration - 400 Epoch - 37 Total training loss - 0.955815564517642 \n",
      "Iteration - 500 Epoch - 37 Total training loss - 1.0038997413798199 \n",
      "Iteration - 600 Epoch - 37 Total training loss - 1.0016114688835913 \n",
      "Iteration - 0 Epoch - 37 Loss - 0.6291795372962952\n",
      "Iteration - 100 Epoch - 37 Loss - 0.9762831100142828\n",
      "Iteration - 200 Epoch - 37 Loss - 0.9745845207527503\n",
      "Iteration - 300 Epoch - 37 Loss - 0.9652694871160279\n",
      "Iteration - 400 Epoch - 37 Loss - 0.9851578002484362\n",
      "Iteration - 500 Epoch - 37 Loss - 0.9823273486660388\n",
      "Iteration - 600 Epoch - 37 Loss - 0.9865796378815234\n",
      "Total validation loss 0.9855849042783936 after 37 epochs\n",
      "Iteration - 0 Epoch - 38 Total training loss - 0.7723393440246582 \n",
      "Iteration - 100 Epoch - 38 Total training loss - 0.9132186862608962 \n",
      "Iteration - 200 Epoch - 38 Total training loss - 0.9760969468321192 \n",
      "Iteration - 300 Epoch - 38 Total training loss - 0.9643637269541568 \n",
      "Iteration - 400 Epoch - 38 Total training loss - 0.931020710679907 \n",
      "Iteration - 500 Epoch - 38 Total training loss - 0.9326529076485663 \n",
      "Iteration - 600 Epoch - 38 Total training loss - 0.9795579567218523 \n",
      "Iteration - 0 Epoch - 38 Loss - 0.23994725942611694\n",
      "Iteration - 100 Epoch - 38 Loss - 0.8521801145876399\n",
      "Iteration - 200 Epoch - 38 Loss - 0.8863716532304808\n",
      "Iteration - 300 Epoch - 38 Loss - 0.8939637746104965\n",
      "Iteration - 400 Epoch - 38 Loss - 0.8854027231806829\n",
      "Iteration - 500 Epoch - 38 Loss - 0.8843392669976115\n",
      "Iteration - 600 Epoch - 38 Loss - 0.8886987451284182\n",
      "Total validation loss 0.888708172662546 after 38 epochs\n",
      "Iteration - 0 Epoch - 39 Total training loss - 0.593056321144104 \n",
      "Iteration - 100 Epoch - 39 Total training loss - 0.7566691269673923 \n",
      "Iteration - 200 Epoch - 39 Total training loss - 0.8634717827987167 \n",
      "Iteration - 300 Epoch - 39 Total training loss - 0.8766240682081625 \n",
      "Iteration - 400 Epoch - 39 Total training loss - 0.8501714499402522 \n",
      "Iteration - 500 Epoch - 39 Total training loss - 0.904531343343729 \n",
      "Iteration - 600 Epoch - 39 Total training loss - 0.9149661212818644 \n",
      "Iteration - 0 Epoch - 39 Loss - 1.0979688167572021\n",
      "Iteration - 100 Epoch - 39 Loss - 0.9402115430572245\n",
      "Iteration - 200 Epoch - 39 Loss - 0.9074619105688079\n",
      "Iteration - 300 Epoch - 39 Loss - 0.9143265408658704\n",
      "Iteration - 400 Epoch - 39 Loss - 0.9186514669430077\n",
      "Iteration - 500 Epoch - 39 Loss - 0.9314643648971698\n",
      "Iteration - 600 Epoch - 39 Loss - 0.9277815069699545\n",
      "Total validation loss 0.9314738605058211 after 39 epochs\n",
      "Iteration - 0 Epoch - 40 Total training loss - 1.2902582883834839 \n",
      "Iteration - 100 Epoch - 40 Total training loss - 0.8729174103735402 \n",
      "Iteration - 200 Epoch - 40 Total training loss - 0.7925071192265891 \n",
      "Iteration - 300 Epoch - 40 Total training loss - 0.9945532715139892 \n",
      "Iteration - 400 Epoch - 40 Total training loss - 1.0219855768538715 \n",
      "Iteration - 500 Epoch - 40 Total training loss - 1.0484363163109418 \n",
      "Iteration - 600 Epoch - 40 Total training loss - 1.0220057596527004 \n",
      "Iteration - 0 Epoch - 40 Loss - 0.6837501525878906\n",
      "Iteration - 100 Epoch - 40 Loss - 0.9339014074885019\n",
      "Iteration - 200 Epoch - 40 Loss - 0.9312524860029789\n",
      "Iteration - 300 Epoch - 40 Loss - 0.9541414961763395\n",
      "Iteration - 400 Epoch - 40 Loss - 0.9561051977542868\n",
      "Iteration - 500 Epoch - 40 Loss - 0.9593219540552227\n",
      "Iteration - 600 Epoch - 40 Loss - 0.9655518320554901\n",
      "Total validation loss 0.9696588444447083 after 40 epochs\n",
      "Iteration - 0 Epoch - 41 Total training loss - 0.3191114068031311 \n",
      "Iteration - 100 Epoch - 41 Total training loss - 0.7381325189097978 \n",
      "Iteration - 200 Epoch - 41 Total training loss - 0.8379344665787104 \n",
      "Iteration - 300 Epoch - 41 Total training loss - 0.844207112299529 \n",
      "Iteration - 400 Epoch - 41 Total training loss - 0.9050068463453735 \n",
      "Iteration - 500 Epoch - 41 Total training loss - 0.9128092419564773 \n",
      "Iteration - 600 Epoch - 41 Total training loss - 0.9354912290679285 \n",
      "Iteration - 0 Epoch - 41 Loss - 1.1125189065933228\n",
      "Iteration - 100 Epoch - 41 Loss - 0.9513745796657801\n",
      "Iteration - 200 Epoch - 41 Loss - 0.9254902011693301\n",
      "Iteration - 300 Epoch - 41 Loss - 0.923309032245958\n",
      "Iteration - 400 Epoch - 41 Loss - 0.9398515371151343\n",
      "Iteration - 500 Epoch - 41 Loss - 0.9449050388044895\n",
      "Iteration - 600 Epoch - 41 Loss - 0.9442741183037728\n",
      "Total validation loss 0.950077709269484 after 41 epochs\n",
      "Iteration - 0 Epoch - 42 Total training loss - 0.45796889066696167 \n",
      "Iteration - 100 Epoch - 42 Total training loss - 1.1972951606621431 \n",
      "Iteration - 200 Epoch - 42 Total training loss - 1.077632720249665 \n",
      "Iteration - 300 Epoch - 42 Total training loss - 1.0397634441171757 \n",
      "Iteration - 400 Epoch - 42 Total training loss - 1.0142773557488822 \n",
      "Iteration - 500 Epoch - 42 Total training loss - 1.0034514682175104 \n",
      "Iteration - 600 Epoch - 42 Total training loss - 1.0071993839493172 \n",
      "Iteration - 0 Epoch - 42 Loss - 0.9602361917495728\n",
      "Iteration - 100 Epoch - 42 Loss - 0.8480490248392124\n",
      "Iteration - 200 Epoch - 42 Loss - 0.8396260637549026\n",
      "Iteration - 300 Epoch - 42 Loss - 0.8188864712748812\n",
      "Iteration - 400 Epoch - 42 Loss - 0.8128125735319351\n",
      "Iteration - 500 Epoch - 42 Loss - 0.8091953738449428\n",
      "Iteration - 600 Epoch - 42 Loss - 0.7994343493545982\n",
      "Total validation loss 0.8023905472753508 after 42 epochs\n",
      "Iteration - 0 Epoch - 43 Total training loss - 0.0011369037674739957 \n",
      "Iteration - 100 Epoch - 43 Total training loss - 1.0391542541202028 \n",
      "Iteration - 200 Epoch - 43 Total training loss - 0.9931984991790838 \n",
      "Iteration - 300 Epoch - 43 Total training loss - 0.9797596507342192 \n",
      "Iteration - 400 Epoch - 43 Total training loss - 0.9692803394076115 \n",
      "Iteration - 500 Epoch - 43 Total training loss - 0.968392502577388 \n",
      "Iteration - 600 Epoch - 43 Total training loss - 0.9819909001459614 \n",
      "Iteration - 0 Epoch - 43 Loss - 1.3422813415527344\n",
      "Iteration - 100 Epoch - 43 Loss - 0.9365679506618197\n",
      "Iteration - 200 Epoch - 43 Loss - 0.949382347995369\n",
      "Iteration - 300 Epoch - 43 Loss - 0.9612385155354624\n",
      "Iteration - 400 Epoch - 43 Loss - 0.9448857673088511\n",
      "Iteration - 500 Epoch - 43 Loss - 0.9449015527576982\n",
      "Iteration - 600 Epoch - 43 Loss - 0.9436071189910719\n",
      "Total validation loss 0.9510777675430644 after 43 epochs\n",
      "Iteration - 0 Epoch - 44 Total training loss - 1.0306237936019897 \n",
      "Iteration - 100 Epoch - 44 Total training loss - 0.9835947617931098 \n",
      "Iteration - 200 Epoch - 44 Total training loss - 0.8910294713788962 \n",
      "Iteration - 300 Epoch - 44 Total training loss - 0.8839668031892912 \n",
      "Iteration - 400 Epoch - 44 Total training loss - 0.8882160100189126 \n",
      "Iteration - 500 Epoch - 44 Total training loss - 0.8813281031451518 \n",
      "Iteration - 600 Epoch - 44 Total training loss - 0.9949038847502706 \n",
      "Iteration - 0 Epoch - 44 Loss - 1.1471387147903442\n",
      "Iteration - 100 Epoch - 44 Loss - 1.0905061384828965\n",
      "Iteration - 200 Epoch - 44 Loss - 1.105421132412716\n",
      "Iteration - 300 Epoch - 44 Loss - 1.0664062689606137\n",
      "Iteration - 400 Epoch - 44 Loss - 1.063215647329416\n",
      "Iteration - 500 Epoch - 44 Loss - 1.05763050738328\n",
      "Iteration - 600 Epoch - 44 Loss - 1.0564679539392077\n",
      "Total validation loss 1.0607383204312673 after 44 epochs\n",
      "Iteration - 0 Epoch - 45 Total training loss - 0.742046058177948 \n",
      "Iteration - 100 Epoch - 45 Total training loss - 1.0375386020214767 \n",
      "Iteration - 200 Epoch - 45 Total training loss - 1.0316929254800762 \n",
      "Iteration - 300 Epoch - 45 Total training loss - 1.0484968094219782 \n",
      "Iteration - 400 Epoch - 45 Total training loss - 1.0496755731848018 \n",
      "Iteration - 500 Epoch - 45 Total training loss - 1.0245932749073585 \n",
      "Iteration - 600 Epoch - 45 Total training loss - 1.0426871198051866 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 0 Epoch - 45 Loss - 1.102959156036377\n",
      "Iteration - 100 Epoch - 45 Loss - 0.9986438314513405\n",
      "Iteration - 200 Epoch - 45 Loss - 0.9948219723665892\n",
      "Iteration - 300 Epoch - 45 Loss - 0.9838036688855321\n",
      "Iteration - 400 Epoch - 45 Loss - 0.987344478505508\n",
      "Iteration - 500 Epoch - 45 Loss - 0.9712142240025564\n",
      "Iteration - 600 Epoch - 45 Loss - 0.9783307352597622\n",
      "Total validation loss 0.9733988468543737 after 45 epochs\n",
      "Iteration - 0 Epoch - 46 Total training loss - 0.9021599292755127 \n",
      "Iteration - 100 Epoch - 46 Total training loss - 1.025822326772625 \n",
      "Iteration - 200 Epoch - 46 Total training loss - 1.1115956251209471 \n",
      "Iteration - 300 Epoch - 46 Total training loss - 1.1090407632186892 \n",
      "Iteration - 400 Epoch - 46 Total training loss - 1.0766253329399713 \n",
      "Iteration - 500 Epoch - 46 Total training loss - 1.0698649572141312 \n",
      "Iteration - 600 Epoch - 46 Total training loss - 1.1083973827314462 \n",
      "Iteration - 0 Epoch - 46 Loss - 1.373151183128357\n",
      "Iteration - 100 Epoch - 46 Loss - 1.018500136561913\n",
      "Iteration - 200 Epoch - 46 Loss - 0.9862152383991735\n",
      "Iteration - 300 Epoch - 46 Loss - 0.9686898553490243\n",
      "Iteration - 400 Epoch - 46 Loss - 0.9711653225291103\n",
      "Iteration - 500 Epoch - 46 Loss - 0.9583213951832757\n",
      "Iteration - 600 Epoch - 46 Loss - 0.9630237646933998\n",
      "Total validation loss 0.9684206849943541 after 46 epochs\n",
      "Iteration - 0 Epoch - 47 Total training loss - 1.6293091773986816 \n",
      "Iteration - 100 Epoch - 47 Total training loss - 0.934756582824163 \n",
      "Iteration - 200 Epoch - 47 Total training loss - 0.9714356138838909 \n",
      "Iteration - 300 Epoch - 47 Total training loss - 0.9732149654563882 \n",
      "Iteration - 400 Epoch - 47 Total training loss - 0.9882403202739436 \n",
      "Iteration - 500 Epoch - 47 Total training loss - 0.9998730205046189 \n",
      "Iteration - 600 Epoch - 47 Total training loss - 0.9924070809143744 \n",
      "Iteration - 0 Epoch - 47 Loss - 1.4638242721557617\n",
      "Iteration - 100 Epoch - 47 Loss - 1.0798404063328657\n",
      "Iteration - 200 Epoch - 47 Loss - 1.0585479745965691\n",
      "Iteration - 300 Epoch - 47 Loss - 1.0593300021665042\n",
      "Iteration - 400 Epoch - 47 Loss - 1.044554253134347\n",
      "Iteration - 500 Epoch - 47 Loss - 1.0443391060817266\n",
      "Iteration - 600 Epoch - 47 Loss - 1.0487582240991307\n",
      "Total validation loss 1.0449663974276793 after 47 epochs\n",
      "Iteration - 0 Epoch - 48 Total training loss - 1.0894734859466553 \n",
      "Iteration - 100 Epoch - 48 Total training loss - 1.0745782437567175 \n",
      "Iteration - 200 Epoch - 48 Total training loss - 1.0333209969505868 \n",
      "Iteration - 300 Epoch - 48 Total training loss - 1.0337233781693518 \n",
      "Iteration - 400 Epoch - 48 Total training loss - 1.0337718903658124 \n",
      "Iteration - 500 Epoch - 48 Total training loss - 1.0125459484227826 \n",
      "Iteration - 600 Epoch - 48 Total training loss - 1.0483360438131706 \n",
      "Iteration - 0 Epoch - 48 Loss - 0.7985813021659851\n",
      "Iteration - 100 Epoch - 48 Loss - 0.9990653872194857\n",
      "Iteration - 200 Epoch - 48 Loss - 1.014563918743857\n",
      "Iteration - 300 Epoch - 48 Loss - 0.9840359687557648\n",
      "Iteration - 400 Epoch - 48 Loss - 1.0106482973085378\n",
      "Iteration - 500 Epoch - 48 Loss - 1.0265184568163164\n",
      "Iteration - 600 Epoch - 48 Loss - 1.031257435592657\n",
      "Total validation loss 1.0308020548685524 after 48 epochs\n",
      "Iteration - 0 Epoch - 49 Total training loss - 0.27195507287979126 \n",
      "Iteration - 100 Epoch - 49 Total training loss - 1.0852318457996306 \n",
      "Iteration - 200 Epoch - 49 Total training loss - 1.0369973120022453 \n",
      "Iteration - 300 Epoch - 49 Total training loss - 1.0364612917442098 \n",
      "Iteration - 400 Epoch - 49 Total training loss - 1.0388739120611163 \n",
      "Iteration - 500 Epoch - 49 Total training loss - 1.0208130050082609 \n",
      "Iteration - 600 Epoch - 49 Total training loss - 1.0852584796875635 \n",
      "Iteration - 0 Epoch - 49 Loss - 0.45823416113853455\n",
      "Iteration - 100 Epoch - 49 Loss - 1.0356443619521538\n",
      "Iteration - 200 Epoch - 49 Loss - 0.987246849161772\n",
      "Iteration - 300 Epoch - 49 Loss - 0.9922236839055619\n",
      "Iteration - 400 Epoch - 49 Loss - 0.9751997226593724\n",
      "Iteration - 500 Epoch - 49 Loss - 0.9756337547202607\n",
      "Iteration - 600 Epoch - 49 Loss - 0.9772808480040851\n",
      "Total validation loss 0.9672472262415345 after 49 epochs\n",
      "Iteration - 0 Epoch - 50 Total training loss - 0.9743884205818176 \n",
      "Iteration - 100 Epoch - 50 Total training loss - 1.10369782749273 \n",
      "Iteration - 200 Epoch - 50 Total training loss - 1.129873069518116 \n",
      "Iteration - 300 Epoch - 50 Total training loss - 1.1342866487574916 \n",
      "Iteration - 400 Epoch - 50 Total training loss - 1.0698346138738826 \n",
      "Iteration - 500 Epoch - 50 Total training loss - 1.1503023330348847 \n",
      "Iteration - 600 Epoch - 50 Total training loss - 1.1628523330766556 \n",
      "Iteration - 0 Epoch - 50 Loss - 0.8324785232543945\n",
      "Iteration - 100 Epoch - 50 Loss - 1.023749828928768\n",
      "Iteration - 200 Epoch - 50 Loss - 0.9983534387762274\n",
      "Iteration - 300 Epoch - 50 Loss - 1.0010979777802265\n",
      "Iteration - 400 Epoch - 50 Loss - 1.0016961205295494\n",
      "Iteration - 500 Epoch - 50 Loss - 0.9989139106786418\n",
      "Iteration - 600 Epoch - 50 Loss - 0.9865983620335576\n",
      "Total validation loss 0.984064281971893 after 50 epochs\n",
      "Iteration - 0 Epoch - 51 Total training loss - 0.2878915071487427 \n",
      "Iteration - 100 Epoch - 51 Total training loss - 0.9784772165155956 \n",
      "Iteration - 200 Epoch - 51 Total training loss - 1.0470827825496898 \n",
      "Iteration - 300 Epoch - 51 Total training loss - 1.0711329743602396 \n",
      "Iteration - 400 Epoch - 51 Total training loss - 1.0580449863344963 \n",
      "Iteration - 500 Epoch - 51 Total training loss - 1.0311185174931037 \n",
      "Iteration - 600 Epoch - 51 Total training loss - 1.0491183037780432 \n",
      "Iteration - 0 Epoch - 51 Loss - 1.7102771997451782\n",
      "Iteration - 100 Epoch - 51 Loss - 1.0879570648516759\n",
      "Iteration - 200 Epoch - 51 Loss - 1.0767025867134183\n",
      "Iteration - 300 Epoch - 51 Loss - 1.0812178091501872\n",
      "Iteration - 400 Epoch - 51 Loss - 1.0797993160802823\n",
      "Iteration - 500 Epoch - 51 Loss - 1.0981788616664634\n",
      "Iteration - 600 Epoch - 51 Loss - 1.1046536275605592\n",
      "Total validation loss 1.1018523417503818 after 51 epochs\n",
      "Iteration - 0 Epoch - 52 Total training loss - 0.29902106523513794 \n",
      "Iteration - 100 Epoch - 52 Total training loss - 0.859864593732475 \n",
      "Iteration - 200 Epoch - 52 Total training loss - 0.9191966714031661 \n",
      "Iteration - 300 Epoch - 52 Total training loss - 0.9544591566058884 \n",
      "Iteration - 400 Epoch - 52 Total training loss - 1.009728035759506 \n",
      "Iteration - 500 Epoch - 52 Total training loss - 1.0502924368123778 \n",
      "Iteration - 600 Epoch - 52 Total training loss - 1.1094736218180883 \n",
      "Iteration - 0 Epoch - 52 Loss - 1.1103278398513794\n",
      "Iteration - 100 Epoch - 52 Loss - 1.0923364876520516\n",
      "Iteration - 200 Epoch - 52 Loss - 1.0742634087059628\n",
      "Iteration - 300 Epoch - 52 Loss - 1.0787797403890034\n",
      "Iteration - 400 Epoch - 52 Loss - 1.0832852057536642\n",
      "Iteration - 500 Epoch - 52 Loss - 1.0670509741859076\n",
      "Iteration - 600 Epoch - 52 Loss - 1.0664702072854273\n",
      "Total validation loss 1.0645079310306866 after 52 epochs\n",
      "Iteration - 0 Epoch - 53 Total training loss - 0.36445102095603943 \n",
      "Iteration - 100 Epoch - 53 Total training loss - 0.9022506392090628 \n",
      "Iteration - 200 Epoch - 53 Total training loss - 1.091651883884449 \n",
      "Iteration - 300 Epoch - 53 Total training loss - 1.0252455942776748 \n",
      "Iteration - 400 Epoch - 53 Total training loss - 1.0207972480582208 \n",
      "Iteration - 500 Epoch - 53 Total training loss - 1.3179784166657305 \n",
      "Iteration - 600 Epoch - 53 Total training loss - 1.2606339182349435 \n",
      "Iteration - 0 Epoch - 53 Loss - 1.608733892440796\n",
      "Iteration - 100 Epoch - 53 Loss - 1.051807389253437\n",
      "Iteration - 200 Epoch - 53 Loss - 1.0561627652959444\n",
      "Iteration - 300 Epoch - 53 Loss - 1.0662607888743727\n",
      "Iteration - 400 Epoch - 53 Loss - 1.080861386924313\n",
      "Iteration - 500 Epoch - 53 Loss - 1.090611523764576\n",
      "Iteration - 600 Epoch - 53 Loss - 1.0962606806078687\n",
      "Total validation loss 1.0872132363063949 after 53 epochs\n",
      "Iteration - 0 Epoch - 54 Total training loss - 1.3563278913497925 \n",
      "Iteration - 100 Epoch - 54 Total training loss - 1.1478472898834118 \n",
      "Iteration - 200 Epoch - 54 Total training loss - 1.0881741086242929 \n",
      "Iteration - 300 Epoch - 54 Total training loss - 1.141097463941764 \n",
      "Iteration - 400 Epoch - 54 Total training loss - 1.0866000269348646 \n",
      "Iteration - 500 Epoch - 54 Total training loss - 1.0591718854986336 \n",
      "Iteration - 600 Epoch - 54 Total training loss - 1.1893417884276094 \n",
      "Iteration - 0 Epoch - 54 Loss - 1.050811529159546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 100 Epoch - 54 Loss - 1.2687351786264098\n",
      "Iteration - 200 Epoch - 54 Loss - 1.2731380219483257\n",
      "Iteration - 300 Epoch - 54 Loss - 1.2625276575056819\n",
      "Iteration - 400 Epoch - 54 Loss - 1.24095743552705\n",
      "Iteration - 500 Epoch - 54 Loss - 1.2343924097910137\n",
      "Iteration - 600 Epoch - 54 Loss - 1.2330597020226985\n",
      "Total validation loss 1.2363155271762627 after 54 epochs\n",
      "Iteration - 0 Epoch - 55 Total training loss - 0.8679770827293396 \n",
      "Iteration - 100 Epoch - 55 Total training loss - 1.1182001849986833 \n",
      "Iteration - 200 Epoch - 55 Total training loss - 1.009910278902644 \n",
      "Iteration - 300 Epoch - 55 Total training loss - 1.1575710604932667 \n",
      "Iteration - 400 Epoch - 55 Total training loss - 1.2841478331023803 \n",
      "Iteration - 500 Epoch - 55 Total training loss - 1.2176656740738039 \n",
      "Iteration - 600 Epoch - 55 Total training loss - 1.1915305958094309 \n",
      "Iteration - 0 Epoch - 55 Loss - 1.3243989944458008\n",
      "Iteration - 100 Epoch - 55 Loss - 1.0976192092541421\n",
      "Iteration - 200 Epoch - 55 Loss - 1.1129728103455026\n",
      "Iteration - 300 Epoch - 55 Loss - 1.1056613064188499\n",
      "Iteration - 400 Epoch - 55 Loss - 1.1038958116288198\n",
      "Iteration - 500 Epoch - 55 Loss - 1.1007917097169244\n",
      "Iteration - 600 Epoch - 55 Loss - 1.096937583632358\n",
      "Total validation loss 1.0914684006587623 after 55 epochs\n",
      "Iteration - 0 Epoch - 56 Total training loss - 1.6972243785858154 \n",
      "Iteration - 100 Epoch - 56 Total training loss - 1.0289635708242064 \n",
      "Iteration - 200 Epoch - 56 Total training loss - 1.037851813951257 \n",
      "Iteration - 300 Epoch - 56 Total training loss - 1.0203418491315503 \n",
      "Iteration - 400 Epoch - 56 Total training loss - 1.014396086583726 \n",
      "Iteration - 500 Epoch - 56 Total training loss - 1.1450007348107891 \n",
      "Iteration - 600 Epoch - 56 Total training loss - 1.1390532496495542 \n",
      "Iteration - 0 Epoch - 56 Loss - 1.2929807901382446\n",
      "Iteration - 100 Epoch - 56 Loss - 1.1661636484141398\n",
      "Iteration - 200 Epoch - 56 Loss - 1.1709887056208368\n",
      "Iteration - 300 Epoch - 56 Loss - 1.148814930769296\n",
      "Iteration - 400 Epoch - 56 Loss - 1.1619355364927924\n",
      "Iteration - 500 Epoch - 56 Loss - 1.1581761766217664\n",
      "Iteration - 600 Epoch - 56 Loss - 1.1703558624187445\n",
      "Total validation loss 1.1634578485506102 after 56 epochs\n",
      "Iteration - 0 Epoch - 57 Total training loss - 1.4319597482681274 \n",
      "Iteration - 100 Epoch - 57 Total training loss - 1.15909550851672 \n",
      "Iteration - 200 Epoch - 57 Total training loss - 1.0140846551393636 \n",
      "Iteration - 300 Epoch - 57 Total training loss - 1.1741820638821432 \n",
      "Iteration - 400 Epoch - 57 Total training loss - 1.1437749043136065 \n",
      "Iteration - 500 Epoch - 57 Total training loss - 1.1242185282337847 \n",
      "Iteration - 600 Epoch - 57 Total training loss - 1.1462959582677588 \n",
      "Iteration - 0 Epoch - 57 Loss - 0.906116783618927\n",
      "Iteration - 100 Epoch - 57 Loss - 1.1096605584763064\n",
      "Iteration - 200 Epoch - 57 Loss - 1.0402001376027492\n",
      "Iteration - 300 Epoch - 57 Loss - 1.0778358165310862\n",
      "Iteration - 400 Epoch - 57 Loss - 1.0800279669853814\n",
      "Iteration - 500 Epoch - 57 Loss - 1.0684766449971113\n",
      "Iteration - 600 Epoch - 57 Loss - 1.0867032623429862\n",
      "Total validation loss 1.0871156798307657 after 57 epochs\n",
      "Iteration - 0 Epoch - 58 Total training loss - 0.8454387187957764 \n",
      "Iteration - 100 Epoch - 58 Total training loss - 1.0624875111681948 \n",
      "Iteration - 200 Epoch - 58 Total training loss - 1.1040036714764834 \n",
      "Iteration - 300 Epoch - 58 Total training loss - 1.2184192713160855 \n",
      "Iteration - 400 Epoch - 58 Total training loss - 1.1917803458504477 \n",
      "Iteration - 500 Epoch - 58 Total training loss - 1.1443942655267068 \n",
      "Iteration - 600 Epoch - 58 Total training loss - 1.1292081271404146 \n",
      "Iteration - 0 Epoch - 58 Loss - 0.8344979286193848\n",
      "Iteration - 100 Epoch - 58 Loss - 0.9267398443552527\n",
      "Iteration - 200 Epoch - 58 Loss - 0.9091994906242807\n",
      "Iteration - 300 Epoch - 58 Loss - 0.9180279707789817\n",
      "Iteration - 400 Epoch - 58 Loss - 0.9217272009932787\n",
      "Iteration - 500 Epoch - 58 Loss - 0.9165671884701281\n",
      "Iteration - 600 Epoch - 58 Loss - 0.9106410012058231\n",
      "Total validation loss 0.9079605652636002 after 58 epochs\n",
      "Iteration - 0 Epoch - 59 Total training loss - 0.3627050220966339 \n",
      "Iteration - 100 Epoch - 59 Total training loss - 1.0609559687684373 \n",
      "Iteration - 200 Epoch - 59 Total training loss - 0.9410447013791446 \n",
      "Iteration - 300 Epoch - 59 Total training loss - 1.0169819073590676 \n",
      "Iteration - 400 Epoch - 59 Total training loss - 0.9929232075787208 \n",
      "Iteration - 500 Epoch - 59 Total training loss - 1.0171418020152254 \n",
      "Iteration - 600 Epoch - 59 Total training loss - 1.0383581588658413 \n",
      "Iteration - 0 Epoch - 59 Loss - 0.5355179309844971\n",
      "Iteration - 100 Epoch - 59 Loss - 1.1210093648716954\n",
      "Iteration - 200 Epoch - 59 Loss - 1.078658445705822\n",
      "Iteration - 300 Epoch - 59 Loss - 1.0862242261634714\n",
      "Iteration - 400 Epoch - 59 Loss - 1.0737959010122422\n",
      "Iteration - 500 Epoch - 59 Loss - 1.06990265673744\n",
      "Iteration - 600 Epoch - 59 Loss - 1.0820612613353475\n",
      "Total validation loss 1.085333556842659 after 59 epochs\n",
      "Iteration - 0 Epoch - 60 Total training loss - 0.9079310297966003 \n",
      "Iteration - 100 Epoch - 60 Total training loss - 1.1540713412041712 \n",
      "Iteration - 200 Epoch - 60 Total training loss - 1.0728270703213132 \n",
      "Iteration - 300 Epoch - 60 Total training loss - 1.1475524029649409 \n",
      "Iteration - 400 Epoch - 60 Total training loss - 1.1465332472287972 \n",
      "Iteration - 500 Epoch - 60 Total training loss - 1.0966703507364566 \n",
      "Iteration - 600 Epoch - 60 Total training loss - 1.0867065469155766 \n",
      "Iteration - 0 Epoch - 60 Loss - 1.2382421493530273\n",
      "Iteration - 100 Epoch - 60 Loss - 1.1767364014493358\n",
      "Iteration - 200 Epoch - 60 Loss - 1.1831406340077149\n",
      "Iteration - 300 Epoch - 60 Loss - 1.180325699704034\n",
      "Iteration - 400 Epoch - 60 Loss - 1.1747012611041938\n",
      "Iteration - 500 Epoch - 60 Loss - 1.163608675707362\n",
      "Iteration - 600 Epoch - 60 Loss - 1.1681531334478725\n",
      "Total validation loss 1.166110898812491 after 60 epochs\n",
      "Iteration - 0 Epoch - 61 Total training loss - 1.6151926517486572 \n",
      "Iteration - 100 Epoch - 61 Total training loss - 1.4428689257960972 \n",
      "Iteration - 200 Epoch - 61 Total training loss - 1.2346423731055731 \n",
      "Iteration - 300 Epoch - 61 Total training loss - 1.1967217674355106 \n",
      "Iteration - 400 Epoch - 61 Total training loss - 1.1703571833501163 \n",
      "Iteration - 500 Epoch - 61 Total training loss - 1.1803141904357688 \n",
      "Iteration - 600 Epoch - 61 Total training loss - 1.1696105806456447 \n",
      "Iteration - 0 Epoch - 61 Loss - 1.4720178842544556\n",
      "Iteration - 100 Epoch - 61 Loss - 1.3480426078975791\n",
      "Iteration - 200 Epoch - 61 Loss - 1.3584292737109152\n",
      "Iteration - 300 Epoch - 61 Loss - 1.3501362237225338\n",
      "Iteration - 400 Epoch - 61 Loss - 1.355698575328413\n",
      "Iteration - 500 Epoch - 61 Loss - 1.337233820837177\n",
      "Iteration - 600 Epoch - 61 Loss - 1.3315175850062124\n",
      "Total validation loss 1.328278858882678 after 61 epochs\n",
      "Iteration - 0 Epoch - 62 Total training loss - 0.8399396538734436 \n",
      "Iteration - 100 Epoch - 62 Total training loss - 1.2718823151847367 \n",
      "Iteration - 200 Epoch - 62 Total training loss - 1.4240044711414837 \n",
      "Iteration - 300 Epoch - 62 Total training loss - 1.5861734513809613 \n",
      "Iteration - 400 Epoch - 62 Total training loss - 1.4632218469126874 \n",
      "Iteration - 500 Epoch - 62 Total training loss - 1.3749313566616088 \n",
      "Iteration - 600 Epoch - 62 Total training loss - 1.3306307492715883 \n",
      "Iteration - 0 Epoch - 62 Loss - 1.4233101606369019\n",
      "Iteration - 100 Epoch - 62 Loss - 1.124607647704606\n",
      "Iteration - 200 Epoch - 62 Loss - 1.1367662021176732\n",
      "Iteration - 300 Epoch - 62 Loss - 1.148215465767439\n",
      "Iteration - 400 Epoch - 62 Loss - 1.1420473856224385\n",
      "Iteration - 500 Epoch - 62 Loss - 1.1410049681891938\n",
      "Iteration - 600 Epoch - 62 Loss - 1.1472494112770886\n",
      "Total validation loss 1.1457894045893544 after 62 epochs\n",
      "Iteration - 0 Epoch - 63 Total training loss - 0.4381296634674072 \n",
      "Iteration - 100 Epoch - 63 Total training loss - 0.7776295424525532 \n",
      "Iteration - 200 Epoch - 63 Total training loss - 1.1239113957993172 \n",
      "Iteration - 300 Epoch - 63 Total training loss - 1.0936317259250448 \n",
      "Iteration - 400 Epoch - 63 Total training loss - 1.1724733905398794 \n",
      "Iteration - 500 Epoch - 63 Total training loss - 1.1594108878808709 \n",
      "Iteration - 600 Epoch - 63 Total training loss - 1.2675429194986676 \n",
      "Iteration - 0 Epoch - 63 Loss - 1.586897373199463\n",
      "Iteration - 100 Epoch - 63 Loss - 1.1751998211487684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 200 Epoch - 63 Loss - 1.1615372497347456\n",
      "Iteration - 300 Epoch - 63 Loss - 1.1692084579966788\n",
      "Iteration - 400 Epoch - 63 Loss - 1.1762158716111408\n",
      "Iteration - 500 Epoch - 63 Loss - 1.1786666380431123\n",
      "Iteration - 600 Epoch - 63 Loss - 1.1848434591650367\n",
      "Total validation loss 1.18760709106741 after 63 epochs\n",
      "Iteration - 0 Epoch - 64 Total training loss - 0.7347611784934998 \n",
      "Iteration - 100 Epoch - 64 Total training loss - 1.0694724303635252 \n",
      "Iteration - 200 Epoch - 64 Total training loss - 1.180374557605174 \n",
      "Iteration - 300 Epoch - 64 Total training loss - 1.1378667607718476 \n",
      "Iteration - 400 Epoch - 64 Total training loss - 1.1704178866412869 \n",
      "Iteration - 500 Epoch - 64 Total training loss - 1.1997809953017624 \n",
      "Iteration - 600 Epoch - 64 Total training loss - 1.1624319578376967 \n",
      "Iteration - 0 Epoch - 64 Loss - 1.2781809568405151\n",
      "Iteration - 100 Epoch - 64 Loss - 1.179548332891842\n",
      "Iteration - 200 Epoch - 64 Loss - 1.140136235685491\n",
      "Iteration - 300 Epoch - 64 Loss - 1.126475994670114\n",
      "Iteration - 400 Epoch - 64 Loss - 1.1250846025503782\n",
      "Iteration - 500 Epoch - 64 Loss - 1.1112854028057433\n",
      "Iteration - 600 Epoch - 64 Loss - 1.113111203303948\n",
      "Total validation loss 1.1116447084580512 after 64 epochs\n",
      "Iteration - 0 Epoch - 65 Total training loss - 0.556189239025116 \n",
      "Iteration - 100 Epoch - 65 Total training loss - 1.1192507375422738 \n",
      "Iteration - 200 Epoch - 65 Total training loss - 1.1921034686045695 \n",
      "Iteration - 300 Epoch - 65 Total training loss - 1.2299068808506581 \n",
      "Iteration - 400 Epoch - 65 Total training loss - 1.3651720330963144 \n",
      "Iteration - 500 Epoch - 65 Total training loss - 1.3399972383496912 \n",
      "Iteration - 600 Epoch - 65 Total training loss - 1.312973142408646 \n",
      "Iteration - 0 Epoch - 65 Loss - 0.8869091868400574\n",
      "Iteration - 100 Epoch - 65 Loss - 1.1825604099448364\n",
      "Iteration - 200 Epoch - 65 Loss - 1.194853323757352\n",
      "Iteration - 300 Epoch - 65 Loss - 1.1965391392327622\n",
      "Iteration - 400 Epoch - 65 Loss - 1.2045326811863002\n",
      "Iteration - 500 Epoch - 65 Loss - 1.210330927919247\n",
      "Iteration - 600 Epoch - 65 Loss - 1.2118395619503473\n",
      "Total validation loss 1.2141903905158347 after 65 epochs\n",
      "Iteration - 0 Epoch - 66 Total training loss - 1.0194867849349976 \n",
      "Iteration - 100 Epoch - 66 Total training loss - 1.0289815430999687 \n",
      "Iteration - 200 Epoch - 66 Total training loss - 1.285587458236523 \n",
      "Iteration - 300 Epoch - 66 Total training loss - 1.2878634112320866 \n",
      "Iteration - 400 Epoch - 66 Total training loss - 1.3200914777276522 \n",
      "Iteration - 500 Epoch - 66 Total training loss - 1.2524733311996727 \n",
      "Iteration - 600 Epoch - 66 Total training loss - 1.294512587805556 \n",
      "Iteration - 0 Epoch - 66 Loss - 0.9961737394332886\n",
      "Iteration - 100 Epoch - 66 Loss - 1.093919576984821\n",
      "Iteration - 200 Epoch - 66 Loss - 1.075423000612069\n",
      "Iteration - 300 Epoch - 66 Loss - 1.1040860623418294\n",
      "Iteration - 400 Epoch - 66 Loss - 1.1082336494601575\n",
      "Iteration - 500 Epoch - 66 Loss - 1.0894405715122908\n",
      "Iteration - 600 Epoch - 66 Loss - 1.096180115335198\n",
      "Total validation loss 1.096059299212821 after 66 epochs\n",
      "Iteration - 0 Epoch - 67 Total training loss - 2.404461145401001 \n",
      "Iteration - 100 Epoch - 67 Total training loss - 1.2077420187057881 \n",
      "Iteration - 200 Epoch - 67 Total training loss - 1.131867906267491 \n",
      "Iteration - 300 Epoch - 67 Total training loss - 1.1645006538874743 \n",
      "Iteration - 400 Epoch - 67 Total training loss - 1.1839004334704581 \n",
      "Iteration - 500 Epoch - 67 Total training loss - 1.1574486068920615 \n",
      "Iteration - 600 Epoch - 67 Total training loss - 1.1252210792334156 \n",
      "Iteration - 0 Epoch - 67 Loss - 1.1312999725341797\n",
      "Iteration - 100 Epoch - 67 Loss - 1.2579956113702\n",
      "Iteration - 200 Epoch - 67 Loss - 1.2365243439354114\n",
      "Iteration - 300 Epoch - 67 Loss - 1.2385735714752413\n",
      "Iteration - 400 Epoch - 67 Loss - 1.2435911341646961\n",
      "Iteration - 500 Epoch - 67 Loss - 1.2431296426022125\n",
      "Iteration - 600 Epoch - 67 Loss - 1.2399919589426671\n",
      "Total validation loss 1.2399672199165206 after 67 epochs\n",
      "Iteration - 0 Epoch - 68 Total training loss - 1.1220860481262207 \n",
      "Iteration - 100 Epoch - 68 Total training loss - 1.0156163911491927 \n",
      "Iteration - 200 Epoch - 68 Total training loss - 0.9851177348222454 \n",
      "Iteration - 300 Epoch - 68 Total training loss - 1.0226135920473889 \n",
      "Iteration - 400 Epoch - 68 Total training loss - 1.3782330314063977 \n",
      "Iteration - 500 Epoch - 68 Total training loss - 1.3696553975269 \n",
      "Iteration - 600 Epoch - 68 Total training loss - 1.3220682789935454 \n",
      "Iteration - 0 Epoch - 68 Loss - 0.6780648827552795\n",
      "Iteration - 100 Epoch - 68 Loss - 1.0639173146521692\n",
      "Iteration - 200 Epoch - 68 Loss - 1.0414387174506685\n",
      "Iteration - 300 Epoch - 68 Loss - 1.0277576090093468\n",
      "Iteration - 400 Epoch - 68 Loss - 1.0369381880819648\n",
      "Iteration - 500 Epoch - 68 Loss - 1.0443934963164692\n",
      "Iteration - 600 Epoch - 68 Loss - 1.036685826191787\n",
      "Total validation loss 1.0433375556531468 after 68 epochs\n",
      "Iteration - 0 Epoch - 69 Total training loss - 0.6010378003120422 \n",
      "Iteration - 100 Epoch - 69 Total training loss - 1.1114352891996209 \n",
      "Iteration - 200 Epoch - 69 Total training loss - 1.1883896954383109 \n",
      "Iteration - 300 Epoch - 69 Total training loss - 1.139746710298385 \n",
      "Iteration - 400 Epoch - 69 Total training loss - 1.164434715423517 \n",
      "Iteration - 500 Epoch - 69 Total training loss - 1.1533376564596596 \n",
      "Iteration - 600 Epoch - 69 Total training loss - 1.1684359216451654 \n",
      "Iteration - 0 Epoch - 69 Loss - 0.9615238308906555\n",
      "Iteration - 100 Epoch - 69 Loss - 1.031605813172784\n",
      "Iteration - 200 Epoch - 69 Loss - 1.0802947628260844\n",
      "Iteration - 300 Epoch - 69 Loss - 1.072436815926007\n",
      "Iteration - 400 Epoch - 69 Loss - 1.0619052334542287\n",
      "Iteration - 500 Epoch - 69 Loss - 1.0593640973527512\n",
      "Iteration - 600 Epoch - 69 Loss - 1.0570692103882995\n",
      "Total validation loss 1.060030353716985 after 69 epochs\n",
      "Iteration - 0 Epoch - 70 Total training loss - 0.9266309142112732 \n",
      "Iteration - 100 Epoch - 70 Total training loss - 1.0239372621881286 \n",
      "Iteration - 200 Epoch - 70 Total training loss - 0.994835268270114 \n",
      "Iteration - 300 Epoch - 70 Total training loss - 1.1555322764293108 \n",
      "Iteration - 400 Epoch - 70 Total training loss - 1.1509913395742084 \n",
      "Iteration - 500 Epoch - 70 Total training loss - 1.1149770459651118 \n",
      "Iteration - 600 Epoch - 70 Total training loss - 1.1668894014493403 \n",
      "Iteration - 0 Epoch - 70 Loss - 0.9042072296142578\n",
      "Iteration - 100 Epoch - 70 Loss - 1.1062002869525758\n",
      "Iteration - 200 Epoch - 70 Loss - 1.0771835691003657\n",
      "Iteration - 300 Epoch - 70 Loss - 1.0665494874862342\n",
      "Iteration - 400 Epoch - 70 Loss - 1.064699276724361\n",
      "Iteration - 500 Epoch - 70 Loss - 1.0658466855803888\n",
      "Iteration - 600 Epoch - 70 Loss - 1.0691352990423384\n",
      "Total validation loss 1.0745063409616882 after 70 epochs\n",
      "Iteration - 0 Epoch - 71 Total training loss - 0.019989997148513794 \n",
      "Iteration - 100 Epoch - 71 Total training loss - 0.9713081414690288 \n",
      "Iteration - 200 Epoch - 71 Total training loss - 0.9093346992951808 \n",
      "Iteration - 300 Epoch - 71 Total training loss - 0.9732517508671548 \n",
      "Iteration - 400 Epoch - 71 Total training loss - 1.0257585261657514 \n",
      "Iteration - 500 Epoch - 71 Total training loss - 1.041758592815066 \n",
      "Iteration - 600 Epoch - 71 Total training loss - 1.0733180888015408 \n",
      "Iteration - 0 Epoch - 71 Loss - 1.5419713258743286\n",
      "Iteration - 100 Epoch - 71 Loss - 1.2088196478267708\n",
      "Iteration - 200 Epoch - 71 Loss - 1.1906078652066379\n",
      "Iteration - 300 Epoch - 71 Loss - 1.1839519105480358\n",
      "Iteration - 400 Epoch - 71 Loss - 1.1917148675407256\n",
      "Iteration - 500 Epoch - 71 Loss - 1.1941972236314458\n",
      "Iteration - 600 Epoch - 71 Loss - 1.1962710379463266\n",
      "Total validation loss 1.1998926682403386 after 71 epochs\n",
      "Iteration - 0 Epoch - 72 Total training loss - 0.8983747363090515 \n",
      "Iteration - 100 Epoch - 72 Total training loss - 1.5543419077328526 \n",
      "Iteration - 200 Epoch - 72 Total training loss - 1.360196267208316 \n",
      "Iteration - 300 Epoch - 72 Total training loss - 1.206493569689608 \n",
      "Iteration - 400 Epoch - 72 Total training loss - 1.1531940626486439 \n",
      "Iteration - 500 Epoch - 72 Total training loss - 1.1493902692083953 \n",
      "Iteration - 600 Epoch - 72 Total training loss - 1.1399027669465798 \n",
      "Iteration - 0 Epoch - 72 Loss - 1.7697927951812744\n",
      "Iteration - 100 Epoch - 72 Loss - 1.147762050059172\n",
      "Iteration - 200 Epoch - 72 Loss - 1.1412381818564377\n",
      "Iteration - 300 Epoch - 72 Loss - 1.1390084148789956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 400 Epoch - 72 Loss - 1.1280845689989087\n",
      "Iteration - 500 Epoch - 72 Loss - 1.1332144922987668\n",
      "Iteration - 600 Epoch - 72 Loss - 1.142810333799106\n",
      "Total validation loss 1.1416813939651276 after 72 epochs\n",
      "Iteration - 0 Epoch - 73 Total training loss - 1.1203184127807617 \n",
      "Iteration - 100 Epoch - 73 Total training loss - 1.2158554157583357 \n",
      "Iteration - 200 Epoch - 73 Total training loss - 1.1690174861004883 \n",
      "Iteration - 300 Epoch - 73 Total training loss - 1.6228275056201285 \n",
      "Iteration - 400 Epoch - 73 Total training loss - 1.567003428198611 \n",
      "Iteration - 500 Epoch - 73 Total training loss - 1.6306711291520672 \n",
      "Iteration - 600 Epoch - 73 Total training loss - 1.5307033812238242 \n",
      "Iteration - 0 Epoch - 73 Loss - 1.0150021314620972\n",
      "Iteration - 100 Epoch - 73 Loss - 0.9827074235913777\n",
      "Iteration - 200 Epoch - 73 Loss - 0.9620659724619258\n",
      "Iteration - 300 Epoch - 73 Loss - 0.9623624437621662\n",
      "Iteration - 400 Epoch - 73 Loss - 0.9765696507922729\n",
      "Iteration - 500 Epoch - 73 Loss - 0.9761348145599374\n",
      "Iteration - 600 Epoch - 73 Loss - 0.9780488396667799\n",
      "Total validation loss 0.978696413915661 after 73 epochs\n",
      "Iteration - 0 Epoch - 74 Total training loss - 1.3049434423446655 \n",
      "Iteration - 100 Epoch - 74 Total training loss - 1.021200530858028 \n",
      "Iteration - 200 Epoch - 74 Total training loss - 1.0383692371985638 \n",
      "Iteration - 300 Epoch - 74 Total training loss - 1.0610833918564393 \n",
      "Iteration - 400 Epoch - 74 Total training loss - 1.1203479096756497 \n",
      "Iteration - 500 Epoch - 74 Total training loss - 1.1499533532170005 \n",
      "Iteration - 600 Epoch - 74 Total training loss - 1.3400521381420605 \n",
      "Iteration - 0 Epoch - 74 Loss - 1.2873156070709229\n",
      "Iteration - 100 Epoch - 74 Loss - 0.9709719997821468\n",
      "Iteration - 200 Epoch - 74 Loss - 1.0154447270863092\n",
      "Iteration - 300 Epoch - 74 Loss - 1.0161146820581632\n",
      "Iteration - 400 Epoch - 74 Loss - 1.0069925639711024\n",
      "Iteration - 500 Epoch - 74 Loss - 1.0172482868405561\n",
      "Iteration - 600 Epoch - 74 Loss - 1.010407842208357\n",
      "Total validation loss 1.0132268655636152 after 74 epochs\n",
      "Iteration - 0 Epoch - 75 Total training loss - 0.466960608959198 \n",
      "Iteration - 100 Epoch - 75 Total training loss - 1.2944581276266882 \n",
      "Iteration - 200 Epoch - 75 Total training loss - 1.1705021858586007 \n",
      "Iteration - 300 Epoch - 75 Total training loss - 1.1602283828826838 \n",
      "Iteration - 400 Epoch - 75 Total training loss - 1.2251801902981025 \n",
      "Iteration - 500 Epoch - 75 Total training loss - 1.2147735517547316 \n",
      "Iteration - 600 Epoch - 75 Total training loss - 1.185023443139981 \n",
      "Iteration - 0 Epoch - 75 Loss - 1.3506823778152466\n",
      "Iteration - 100 Epoch - 75 Loss - 0.9997714318261288\n",
      "Iteration - 200 Epoch - 75 Loss - 1.0114090064568306\n",
      "Iteration - 300 Epoch - 75 Loss - 1.0082197706663727\n",
      "Iteration - 400 Epoch - 75 Loss - 1.0060928640817466\n",
      "Iteration - 500 Epoch - 75 Loss - 1.0157856802026668\n",
      "Iteration - 600 Epoch - 75 Loss - 1.0158087241551692\n",
      "Total validation loss 1.0160422680480548 after 75 epochs\n",
      "Iteration - 0 Epoch - 76 Total training loss - 1.3402488231658936 \n",
      "Iteration - 100 Epoch - 76 Total training loss - 1.7062728472865454 \n",
      "Iteration - 200 Epoch - 76 Total training loss - 1.2365309622935692 \n",
      "Iteration - 300 Epoch - 76 Total training loss - 1.2241895956333813 \n",
      "Iteration - 400 Epoch - 76 Total training loss - 1.1901331038441871 \n",
      "Iteration - 500 Epoch - 76 Total training loss - 1.159943415171352 \n",
      "Iteration - 600 Epoch - 76 Total training loss - 1.1872952949471587 \n",
      "Iteration - 0 Epoch - 76 Loss - 1.7172484397888184\n",
      "Iteration - 100 Epoch - 76 Loss - 1.1067386918728894\n",
      "Iteration - 200 Epoch - 76 Loss - 1.0782035919860822\n",
      "Iteration - 300 Epoch - 76 Loss - 1.088099265019363\n",
      "Iteration - 400 Epoch - 76 Loss - 1.0786876281597966\n",
      "Iteration - 500 Epoch - 76 Loss - 1.0648677311614603\n",
      "Iteration - 600 Epoch - 76 Loss - 1.065260356691931\n",
      "Total validation loss 1.0644184171949658 after 76 epochs\n",
      "Iteration - 0 Epoch - 77 Total training loss - 3.9155168533325195 \n",
      "Iteration - 100 Epoch - 77 Total training loss - 1.2828044455941066 \n",
      "Iteration - 200 Epoch - 77 Total training loss - 1.1180278307559033 \n",
      "Iteration - 300 Epoch - 77 Total training loss - 1.1524152766937927 \n",
      "Iteration - 400 Epoch - 77 Total training loss - 1.1755426946314438 \n",
      "Iteration - 500 Epoch - 77 Total training loss - 1.1502390184402307 \n",
      "Iteration - 600 Epoch - 77 Total training loss - 1.1588253892004954 \n",
      "Iteration - 0 Epoch - 77 Loss - 1.1991174221038818\n",
      "Iteration - 100 Epoch - 77 Loss - 0.9197771434146579\n",
      "Iteration - 200 Epoch - 77 Loss - 0.8870153905295614\n",
      "Iteration - 300 Epoch - 77 Loss - 0.878311823892633\n",
      "Iteration - 400 Epoch - 77 Loss - 0.8679036666961977\n",
      "Iteration - 500 Epoch - 77 Loss - 0.868773425068565\n",
      "Iteration - 600 Epoch - 77 Loss - 0.8706491040980161\n",
      "Total validation loss 0.8693433170826783 after 77 epochs\n",
      "Iteration - 0 Epoch - 78 Total training loss - 0.7778970003128052 \n",
      "Iteration - 100 Epoch - 78 Total training loss - 1.1464012548567193 \n",
      "Iteration - 200 Epoch - 78 Total training loss - 1.4053756074148198 \n",
      "Iteration - 300 Epoch - 78 Total training loss - 1.3520961261247642 \n",
      "Iteration - 400 Epoch - 78 Total training loss - 1.2752545335640122 \n",
      "Iteration - 500 Epoch - 78 Total training loss - 1.22765280414421 \n",
      "Iteration - 600 Epoch - 78 Total training loss - 1.2156599616210677 \n",
      "Iteration - 0 Epoch - 78 Loss - 1.3177028894424438\n",
      "Iteration - 100 Epoch - 78 Loss - 1.1280764190867396\n",
      "Iteration - 200 Epoch - 78 Loss - 1.0881280641057598\n",
      "Iteration - 300 Epoch - 78 Loss - 1.097918950937119\n",
      "Iteration - 400 Epoch - 78 Loss - 1.1253994522695232\n",
      "Iteration - 500 Epoch - 78 Loss - 1.137929396833964\n",
      "Iteration - 600 Epoch - 78 Loss - 1.1361257511992224\n",
      "Total validation loss 1.1426508517024365 after 78 epochs\n",
      "Iteration - 0 Epoch - 79 Total training loss - 7.475332260131836 \n",
      "Iteration - 100 Epoch - 79 Total training loss - 1.2865189245948394 \n",
      "Iteration - 200 Epoch - 79 Total training loss - 1.3725334765950354 \n",
      "Iteration - 300 Epoch - 79 Total training loss - 1.2957856849173781 \n",
      "Iteration - 400 Epoch - 79 Total training loss - 1.2741797448610137 \n",
      "Iteration - 500 Epoch - 79 Total training loss - 1.2469242701595422 \n",
      "Iteration - 600 Epoch - 79 Total training loss - 1.236673209292978 \n",
      "Iteration - 0 Epoch - 79 Loss - 0.7987182140350342\n",
      "Iteration - 100 Epoch - 79 Loss - 1.1229892677009696\n",
      "Iteration - 200 Epoch - 79 Loss - 1.172368433493287\n",
      "Iteration - 300 Epoch - 79 Loss - 1.173472670423628\n",
      "Iteration - 400 Epoch - 79 Loss - 1.1647922650091072\n",
      "Iteration - 500 Epoch - 79 Loss - 1.1692138809167933\n",
      "Iteration - 600 Epoch - 79 Loss - 1.1774576154604133\n",
      "Total validation loss 1.1793456883778326 after 79 epochs\n",
      "Iteration - 0 Epoch - 80 Total training loss - 0.6103021502494812 \n",
      "Iteration - 100 Epoch - 80 Total training loss - 1.084543058925336 \n",
      "Iteration - 200 Epoch - 80 Total training loss - 1.169377272686833 \n",
      "Iteration - 300 Epoch - 80 Total training loss - 1.1730426097019275 \n",
      "Iteration - 400 Epoch - 80 Total training loss - 1.2352862677389227 \n",
      "Iteration - 500 Epoch - 80 Total training loss - 1.2623127693617873 \n",
      "Iteration - 600 Epoch - 80 Total training loss - 1.2345613888192777 \n",
      "Iteration - 0 Epoch - 80 Loss - 1.3076361417770386\n",
      "Iteration - 100 Epoch - 80 Loss - 1.3848844139587762\n",
      "Iteration - 200 Epoch - 80 Loss - 1.358146991821664\n",
      "Iteration - 300 Epoch - 80 Loss - 1.361929483340428\n",
      "Iteration - 400 Epoch - 80 Loss - 1.410214650920799\n",
      "Iteration - 500 Epoch - 80 Loss - 1.408274289822864\n",
      "Iteration - 600 Epoch - 80 Loss - 1.413441402121709\n",
      "Total validation loss 1.417484746402339 after 80 epochs\n",
      "Iteration - 0 Epoch - 81 Total training loss - 2.7058281898498535 \n",
      "Iteration - 100 Epoch - 81 Total training loss - 1.2224320269069902 \n",
      "Iteration - 200 Epoch - 81 Total training loss - 1.1960246025987749 \n",
      "Iteration - 300 Epoch - 81 Total training loss - 1.2595558757166176 \n",
      "Iteration - 400 Epoch - 81 Total training loss - 1.1657226175537452 \n",
      "Iteration - 500 Epoch - 81 Total training loss - 1.239195163575013 \n",
      "Iteration - 600 Epoch - 81 Total training loss - 1.288750965448427 \n",
      "Iteration - 0 Epoch - 81 Loss - 0.6368144154548645\n",
      "Iteration - 100 Epoch - 81 Loss - 1.0852729517339479\n",
      "Iteration - 200 Epoch - 81 Loss - 1.0747957713067977\n",
      "Iteration - 300 Epoch - 81 Loss - 1.081590584698953\n",
      "Iteration - 400 Epoch - 81 Loss - 1.0821603750322302\n",
      "Iteration - 500 Epoch - 81 Loss - 1.0852391872436047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 600 Epoch - 81 Loss - 1.0758046071245002\n",
      "Total validation loss 1.0819579825558243 after 81 epochs\n",
      "Iteration - 0 Epoch - 82 Total training loss - 0.18910154700279236 \n",
      "Iteration - 100 Epoch - 82 Total training loss - 1.433296432294468 \n",
      "Iteration - 200 Epoch - 82 Total training loss - 1.2734695108282637 \n",
      "Iteration - 300 Epoch - 82 Total training loss - 1.2240588463347524 \n",
      "Iteration - 400 Epoch - 82 Total training loss - 1.2314150948519078 \n",
      "Iteration - 500 Epoch - 82 Total training loss - 1.1991198837874513 \n",
      "Iteration - 600 Epoch - 82 Total training loss - 1.172225862531327 \n",
      "Iteration - 0 Epoch - 82 Loss - 0.8850122690200806\n",
      "Iteration - 100 Epoch - 82 Loss - 1.1585187280532157\n",
      "Iteration - 200 Epoch - 82 Loss - 1.1781747550217074\n",
      "Iteration - 300 Epoch - 82 Loss - 1.644247798230561\n",
      "Iteration - 400 Epoch - 82 Loss - 1.8483283313878456\n",
      "Iteration - 500 Epoch - 82 Loss - 1.7389731604657963\n",
      "Iteration - 600 Epoch - 82 Loss - 1.6487613710607347\n",
      "Total validation loss 1.6131555093367411 after 82 epochs\n",
      "Iteration - 0 Epoch - 83 Total training loss - 1.7952877283096313 \n",
      "Iteration - 100 Epoch - 83 Total training loss - 8.975461949935497 \n",
      "Iteration - 200 Epoch - 83 Total training loss - 5.034505876355726 \n",
      "Iteration - 300 Epoch - 83 Total training loss - 4.010340821520469 \n",
      "Iteration - 400 Epoch - 83 Total training loss - 3.306668518015597 \n",
      "Iteration - 500 Epoch - 83 Total training loss - 2.882804202178361 \n",
      "Iteration - 600 Epoch - 83 Total training loss - 2.650923158668507 \n",
      "Iteration - 0 Epoch - 83 Loss - 0.9640987515449524\n",
      "Iteration - 100 Epoch - 83 Loss - 1.0133507709101874\n",
      "Iteration - 200 Epoch - 83 Loss - 1.0518109695383566\n",
      "Iteration - 300 Epoch - 83 Loss - 1.0674855434320298\n",
      "Iteration - 400 Epoch - 83 Loss - 1.0687324770742521\n",
      "Iteration - 500 Epoch - 83 Loss - 1.0789591955864977\n",
      "Iteration - 600 Epoch - 83 Loss - 1.079843837662465\n",
      "Total validation loss 1.0801200680321472 after 83 epochs\n",
      "Iteration - 0 Epoch - 84 Total training loss - 1.3947057723999023 \n",
      "Iteration - 100 Epoch - 84 Total training loss - 1.156892013062935 \n",
      "Iteration - 200 Epoch - 84 Total training loss - 1.0960917335504026 \n",
      "Iteration - 300 Epoch - 84 Total training loss - 1.0960217199707196 \n",
      "Iteration - 400 Epoch - 84 Total training loss - 1.1060247943291293 \n",
      "Iteration - 500 Epoch - 84 Total training loss - 1.1769280171117105 \n",
      "Iteration - 600 Epoch - 84 Total training loss - 1.3033576745315736 \n",
      "Iteration - 0 Epoch - 84 Loss - 1.2532739639282227\n",
      "Iteration - 100 Epoch - 84 Loss - 1.1289674570359807\n",
      "Iteration - 200 Epoch - 84 Loss - 1.0970534442817395\n",
      "Iteration - 300 Epoch - 84 Loss - 1.097174463428532\n",
      "Iteration - 400 Epoch - 84 Loss - 1.0977160783108333\n",
      "Iteration - 500 Epoch - 84 Loss - 1.0989977770193133\n",
      "Iteration - 600 Epoch - 84 Loss - 1.1033036293161291\n",
      "Total validation loss 1.1022449424178948 after 84 epochs\n",
      "Iteration - 0 Epoch - 85 Total training loss - 1.1039817333221436 \n",
      "Iteration - 100 Epoch - 85 Total training loss - 0.9938116518799032 \n",
      "Iteration - 200 Epoch - 85 Total training loss - 1.028096815144912 \n",
      "Iteration - 300 Epoch - 85 Total training loss - 1.1267152969412926 \n",
      "Iteration - 400 Epoch - 85 Total training loss - 1.218401747483974 \n",
      "Iteration - 500 Epoch - 85 Total training loss - 1.207180527150535 \n",
      "Iteration - 600 Epoch - 85 Total training loss - 1.2220645050862766 \n",
      "Iteration - 0 Epoch - 85 Loss - 2.1579527854919434\n",
      "Iteration - 100 Epoch - 85 Loss - 1.0333488853556094\n",
      "Iteration - 200 Epoch - 85 Loss - 1.0342489376627093\n",
      "Iteration - 300 Epoch - 85 Loss - 1.0597058305535403\n",
      "Iteration - 400 Epoch - 85 Loss - 1.059836647577788\n",
      "Iteration - 500 Epoch - 85 Loss - 1.0569315310381724\n",
      "Iteration - 600 Epoch - 85 Loss - 1.0498357410054535\n",
      "Total validation loss 1.0505966619619573 after 85 epochs\n",
      "Iteration - 0 Epoch - 86 Total training loss - 2.063493251800537 \n",
      "Iteration - 100 Epoch - 86 Total training loss - 1.1797886185421802 \n",
      "Iteration - 200 Epoch - 86 Total training loss - 1.2200405711484192 \n",
      "Iteration - 300 Epoch - 86 Total training loss - 1.1812641925435279 \n",
      "Iteration - 400 Epoch - 86 Total training loss - 1.2495636650595694 \n",
      "Iteration - 500 Epoch - 86 Total training loss - 1.2599714691704231 \n",
      "Iteration - 600 Epoch - 86 Total training loss - 1.2445000940809854 \n",
      "Iteration - 0 Epoch - 86 Loss - 1.0253955125808716\n",
      "Iteration - 100 Epoch - 86 Loss - 1.397834024807014\n",
      "Iteration - 200 Epoch - 86 Loss - 1.381764957264288\n",
      "Iteration - 300 Epoch - 86 Loss - 1.3741444549786292\n",
      "Iteration - 400 Epoch - 86 Loss - 1.3716241991207785\n",
      "Iteration - 500 Epoch - 86 Loss - 1.3639837958201202\n",
      "Iteration - 600 Epoch - 86 Loss - 1.353639086624747\n",
      "Total validation loss 1.359570710107367 after 86 epochs\n",
      "Iteration - 0 Epoch - 87 Total training loss - 1.5539534091949463 \n",
      "Iteration - 100 Epoch - 87 Total training loss - 1.8780096188777744 \n",
      "Iteration - 200 Epoch - 87 Total training loss - 1.404570665747965 \n",
      "Iteration - 300 Epoch - 87 Total training loss - 1.4106558706701124 \n",
      "Iteration - 400 Epoch - 87 Total training loss - 1.4038276911406153 \n",
      "Iteration - 500 Epoch - 87 Total training loss - 1.44665709931396 \n",
      "Iteration - 600 Epoch - 87 Total training loss - 1.433957576126406 \n",
      "Iteration - 0 Epoch - 87 Loss - 0.9057459831237793\n",
      "Iteration - 100 Epoch - 87 Loss - 1.168239322481769\n",
      "Iteration - 200 Epoch - 87 Loss - 1.2225117827974148\n",
      "Iteration - 300 Epoch - 87 Loss - 1.2188375670945526\n",
      "Iteration - 400 Epoch - 87 Loss - 1.2306255342582813\n",
      "Iteration - 500 Epoch - 87 Loss - 1.2245615177347275\n",
      "Iteration - 600 Epoch - 87 Loss - 1.2304942678294444\n",
      "Total validation loss 1.2299735092283381 after 87 epochs\n",
      "Iteration - 0 Epoch - 88 Total training loss - 1.9071788787841797 \n",
      "Iteration - 100 Epoch - 88 Total training loss - 1.3062793992386006 \n",
      "Iteration - 200 Epoch - 88 Total training loss - 1.7134738133858833 \n",
      "Iteration - 300 Epoch - 88 Total training loss - 1.55237150734965 \n",
      "Iteration - 400 Epoch - 88 Total training loss - 1.5444921615795972 \n",
      "Iteration - 500 Epoch - 88 Total training loss - 1.478379621242785 \n",
      "Iteration - 600 Epoch - 88 Total training loss - 1.4029846068488814 \n",
      "Iteration - 0 Epoch - 88 Loss - 1.6386364698410034\n",
      "Iteration - 100 Epoch - 88 Loss - 1.217836719633329\n",
      "Iteration - 200 Epoch - 88 Loss - 1.2060548178295591\n",
      "Iteration - 300 Epoch - 88 Loss - 1.1952207685507017\n",
      "Iteration - 400 Epoch - 88 Loss - 1.1955615143823504\n",
      "Iteration - 500 Epoch - 88 Loss - 1.1895795684612678\n",
      "Iteration - 600 Epoch - 88 Loss - 1.1809188110657818\n",
      "Total validation loss 1.185992333845527 after 88 epochs\n",
      "Iteration - 0 Epoch - 89 Total training loss - 0.9152272939682007 \n",
      "Iteration - 100 Epoch - 89 Total training loss - 1.1212900773150274 \n",
      "Iteration - 200 Epoch - 89 Total training loss - 1.300278465808274 \n",
      "Iteration - 300 Epoch - 89 Total training loss - 1.3021642915253662 \n",
      "Iteration - 400 Epoch - 89 Total training loss - 1.3271069054381834 \n",
      "Iteration - 500 Epoch - 89 Total training loss - 1.3310981681078993 \n",
      "Iteration - 600 Epoch - 89 Total training loss - 1.368123874363563 \n",
      "Iteration - 0 Epoch - 89 Loss - 0.991295576095581\n",
      "Iteration - 100 Epoch - 89 Loss - 1.18950217018033\n",
      "Iteration - 200 Epoch - 89 Loss - 1.2157421598387002\n",
      "Iteration - 300 Epoch - 89 Loss - 1.2283468722703053\n",
      "Iteration - 400 Epoch - 89 Loss - 1.2128638301852932\n",
      "Iteration - 500 Epoch - 89 Loss - 1.2044983418758757\n",
      "Iteration - 600 Epoch - 89 Loss - 1.2000354670248492\n",
      "Total validation loss 1.2071198665035894 after 89 epochs\n",
      "Iteration - 0 Epoch - 90 Total training loss - 0.5163156390190125 \n",
      "Iteration - 100 Epoch - 90 Total training loss - 1.311475464723999 \n",
      "Iteration - 200 Epoch - 90 Total training loss - 1.505839989854326 \n",
      "Iteration - 300 Epoch - 90 Total training loss - 1.5167382739692339 \n",
      "Iteration - 400 Epoch - 90 Total training loss - 1.4143821559370833 \n",
      "Iteration - 500 Epoch - 90 Total training loss - 1.3728850418748677 \n",
      "Iteration - 600 Epoch - 90 Total training loss - 1.3284012889451153 \n",
      "Iteration - 0 Epoch - 90 Loss - 1.680485486984253\n",
      "Iteration - 100 Epoch - 90 Loss - 1.1666216972735848\n",
      "Iteration - 200 Epoch - 90 Loss - 1.202783642864939\n",
      "Iteration - 300 Epoch - 90 Loss - 1.1714403038207082\n",
      "Iteration - 400 Epoch - 90 Loss - 1.179855614751949\n",
      "Iteration - 500 Epoch - 90 Loss - 1.175247323251294\n",
      "Iteration - 600 Epoch - 90 Loss - 1.1746818065246607\n",
      "Total validation loss 1.1777339069495447 after 90 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - 0 Epoch - 91 Total training loss - 0.16107018291950226 \n",
      "Iteration - 100 Epoch - 91 Total training loss - 1.0073504957029282 \n",
      "Iteration - 200 Epoch - 91 Total training loss - 1.2350982655423568 \n",
      "Iteration - 300 Epoch - 91 Total training loss - 1.2432640005503333 \n",
      "Iteration - 400 Epoch - 91 Total training loss - 1.5233868131520891 \n",
      "Iteration - 500 Epoch - 91 Total training loss - 1.4430982225282702 \n",
      "Iteration - 600 Epoch - 91 Total training loss - 1.3881938295468328 \n",
      "Iteration - 0 Epoch - 91 Loss - 1.2522568702697754\n",
      "Iteration - 100 Epoch - 91 Loss - 1.3560168353637847\n",
      "Iteration - 200 Epoch - 91 Loss - 1.465175547113466\n",
      "Iteration - 300 Epoch - 91 Loss - 1.4357150542577637\n",
      "Iteration - 400 Epoch - 91 Loss - 1.4387223223349697\n",
      "Iteration - 500 Epoch - 91 Loss - 1.427008310596862\n",
      "Iteration - 600 Epoch - 91 Loss - 1.4342407199388336\n",
      "Total validation loss 1.4357589527647545 after 91 epochs\n",
      "Iteration - 0 Epoch - 92 Total training loss - 0.8973705172538757 \n",
      "Iteration - 100 Epoch - 92 Total training loss - 1.7475632350652082 \n",
      "Iteration - 200 Epoch - 92 Total training loss - 1.854955972339124 \n",
      "Iteration - 300 Epoch - 92 Total training loss - 1.612614265011865 \n",
      "Iteration - 400 Epoch - 92 Total training loss - 1.5615177964154419 \n",
      "Iteration - 500 Epoch - 92 Total training loss - 1.4742186108536455 \n",
      "Iteration - 600 Epoch - 92 Total training loss - 1.482612377631663 \n",
      "Iteration - 0 Epoch - 92 Loss - 1.5586071014404297\n",
      "Iteration - 100 Epoch - 92 Loss - 1.6878252351047969\n",
      "Iteration - 200 Epoch - 92 Loss - 1.719885869998837\n",
      "Iteration - 300 Epoch - 92 Loss - 1.69733859395664\n",
      "Iteration - 400 Epoch - 92 Loss - 1.7736537246930035\n",
      "Iteration - 500 Epoch - 92 Loss - 1.750209603659407\n",
      "Iteration - 600 Epoch - 92 Loss - 1.7801885212618183\n",
      "Total validation loss 1.7934900033528318 after 92 epochs\n",
      "Iteration - 0 Epoch - 93 Total training loss - 0.7451719045639038 \n",
      "Iteration - 100 Epoch - 93 Total training loss - 1.2638260416391462 \n",
      "Iteration - 200 Epoch - 93 Total training loss - 1.2520612209331041 \n",
      "Iteration - 300 Epoch - 93 Total training loss - 1.2209580155813515 \n",
      "Iteration - 400 Epoch - 93 Total training loss - 1.2541784587318539 \n",
      "Iteration - 500 Epoch - 93 Total training loss - 1.2284870215555925 \n",
      "Iteration - 600 Epoch - 93 Total training loss - 1.251378007006898 \n",
      "Iteration - 0 Epoch - 93 Loss - 1.0415655374526978\n",
      "Iteration - 100 Epoch - 93 Loss - 1.4080985035046492\n",
      "Iteration - 200 Epoch - 93 Loss - 1.3608766613314993\n",
      "Iteration - 300 Epoch - 93 Loss - 1.3654295728848227\n",
      "Iteration - 400 Epoch - 93 Loss - 1.3683977767713646\n",
      "Iteration - 500 Epoch - 93 Loss - 1.3755878691901704\n",
      "Iteration - 600 Epoch - 93 Loss - 1.3772911219152555\n",
      "Total validation loss 1.3782978970047912 after 93 epochs\n",
      "Iteration - 0 Epoch - 94 Total training loss - 0.45114246010780334 \n",
      "Iteration - 100 Epoch - 94 Total training loss - 1.0812739816558834 \n",
      "Iteration - 200 Epoch - 94 Total training loss - 1.2111324483166166 \n",
      "Iteration - 300 Epoch - 94 Total training loss - 1.1911004441932418 \n",
      "Iteration - 400 Epoch - 94 Total training loss - 1.3053319461162793 \n",
      "Iteration - 500 Epoch - 94 Total training loss - 1.2601452395624628 \n",
      "Iteration - 600 Epoch - 94 Total training loss - 1.2663959631525736 \n",
      "Iteration - 0 Epoch - 94 Loss - 0.6506943702697754\n",
      "Iteration - 100 Epoch - 94 Loss - 1.083905163672891\n",
      "Iteration - 200 Epoch - 94 Loss - 1.0671870067196698\n",
      "Iteration - 300 Epoch - 94 Loss - 1.0662113255341583\n",
      "Iteration - 400 Epoch - 94 Loss - 1.0567703119611502\n",
      "Iteration - 500 Epoch - 94 Loss - 1.0734931148752243\n",
      "Iteration - 600 Epoch - 94 Loss - 1.07440178470683\n",
      "Total validation loss 1.0766832953075507 after 94 epochs\n",
      "Iteration - 0 Epoch - 95 Total training loss - 1.3520405292510986 \n",
      "Iteration - 100 Epoch - 95 Total training loss - 1.1175115431359528 \n",
      "Iteration - 200 Epoch - 95 Total training loss - 1.2229083140091235 \n",
      "Iteration - 300 Epoch - 95 Total training loss - 1.168253311972326 \n",
      "Iteration - 400 Epoch - 95 Total training loss - 1.3137452610177374 \n",
      "Iteration - 500 Epoch - 95 Total training loss - 1.298282573225369 \n",
      "Iteration - 600 Epoch - 95 Total training loss - 1.2492297279288245 \n",
      "Iteration - 0 Epoch - 95 Loss - 1.5196441411972046\n",
      "Iteration - 100 Epoch - 95 Loss - 1.1914985578248996\n",
      "Iteration - 200 Epoch - 95 Loss - 1.1704269807730148\n",
      "Iteration - 300 Epoch - 95 Loss - 1.1976980273905782\n",
      "Iteration - 400 Epoch - 95 Loss - 1.2109312925701428\n",
      "Iteration - 500 Epoch - 95 Loss - 1.209548283241942\n",
      "Iteration - 600 Epoch - 95 Loss - 1.208262046039749\n",
      "Total validation loss 1.2096002186085224 after 95 epochs\n",
      "Iteration - 0 Epoch - 96 Total training loss - 0.30151131749153137 \n",
      "Iteration - 100 Epoch - 96 Total training loss - 1.6959206285969455 \n",
      "Iteration - 200 Epoch - 96 Total training loss - 1.3529607385897369 \n",
      "Iteration - 300 Epoch - 96 Total training loss - 1.3129385760196322 \n",
      "Iteration - 400 Epoch - 96 Total training loss - 1.4812373455191157 \n",
      "Iteration - 500 Epoch - 96 Total training loss - 1.3931983038243985 \n",
      "Iteration - 600 Epoch - 96 Total training loss - 1.3636300853637982 \n",
      "Iteration - 0 Epoch - 96 Loss - 1.7415575981140137\n",
      "Iteration - 100 Epoch - 96 Loss - 1.2533896302822793\n",
      "Iteration - 200 Epoch - 96 Loss - 1.2691491103884\n",
      "Iteration - 300 Epoch - 96 Loss - 1.261865894263765\n",
      "Iteration - 400 Epoch - 96 Loss - 1.2579481743666299\n",
      "Iteration - 500 Epoch - 96 Loss - 1.2378210054483718\n",
      "Iteration - 600 Epoch - 96 Loss - 1.243809122214103\n",
      "Total validation loss 1.2456166614979325 after 96 epochs\n",
      "Iteration - 0 Epoch - 97 Total training loss - 0.9431231021881104 \n",
      "Iteration - 100 Epoch - 97 Total training loss - 1.3174444334120443 \n",
      "Iteration - 200 Epoch - 97 Total training loss - 1.4517470965375059 \n",
      "Iteration - 300 Epoch - 97 Total training loss - 1.406226289115515 \n",
      "Iteration - 400 Epoch - 97 Total training loss - 1.3681914088484062 \n",
      "Iteration - 500 Epoch - 97 Total training loss - 1.3260660831690552 \n",
      "Iteration - 600 Epoch - 97 Total training loss - 1.306070900936115 \n",
      "Iteration - 0 Epoch - 97 Loss - 1.3736354112625122\n",
      "Iteration - 100 Epoch - 97 Loss - 1.297054396997584\n",
      "Iteration - 200 Epoch - 97 Loss - 1.283744691023186\n",
      "Iteration - 300 Epoch - 97 Loss - 1.2686053615471848\n",
      "Iteration - 400 Epoch - 97 Loss - 1.251326260498337\n",
      "Iteration - 500 Epoch - 97 Loss - 1.2484162133967804\n",
      "Iteration - 600 Epoch - 97 Loss - 1.2502360268063633\n",
      "Total validation loss 1.2496533186421206 after 97 epochs\n",
      "Iteration - 0 Epoch - 98 Total training loss - 0.7719942331314087 \n",
      "Iteration - 100 Epoch - 98 Total training loss - 1.5899307036698602 \n",
      "Iteration - 200 Epoch - 98 Total training loss - 1.517156092297119 \n",
      "Iteration - 300 Epoch - 98 Total training loss - 1.477817129875159 \n",
      "Iteration - 400 Epoch - 98 Total training loss - 1.3937844086962141 \n",
      "Iteration - 500 Epoch - 98 Total training loss - 1.8573175697041835 \n",
      "Iteration - 600 Epoch - 98 Total training loss - 1.7156954059348644 \n",
      "Iteration - 0 Epoch - 98 Loss - 0.8088963031768799\n",
      "Iteration - 100 Epoch - 98 Loss - 1.1494980138717312\n",
      "Iteration - 200 Epoch - 98 Loss - 1.1672552904086326\n",
      "Iteration - 300 Epoch - 98 Loss - 1.15043313410195\n",
      "Iteration - 400 Epoch - 98 Loss - 1.1565676147206465\n",
      "Iteration - 500 Epoch - 98 Loss - 1.1550972455157016\n",
      "Iteration - 600 Epoch - 98 Loss - 1.1534781612195508\n",
      "Total validation loss 1.1521969989259193 after 98 epochs\n",
      "Iteration - 0 Epoch - 99 Total training loss - 0.22442081570625305 \n",
      "Iteration - 100 Epoch - 99 Total training loss - 1.201887141009516 \n",
      "Iteration - 200 Epoch - 99 Total training loss - 1.288438668567224 \n",
      "Iteration - 300 Epoch - 99 Total training loss - 1.3732728264704532 \n",
      "Iteration - 400 Epoch - 99 Total training loss - 1.3762620372292436 \n",
      "Iteration - 500 Epoch - 99 Total training loss - 1.3241079124206576 \n",
      "Iteration - 600 Epoch - 99 Total training loss - 1.3124439346341776 \n",
      "Iteration - 0 Epoch - 99 Loss - 0.6227899193763733\n",
      "Iteration - 100 Epoch - 99 Loss - 1.3503993059148882\n",
      "Iteration - 200 Epoch - 99 Loss - 1.3712961693011707\n",
      "Iteration - 300 Epoch - 99 Loss - 1.3613034964993942\n",
      "Iteration - 400 Epoch - 99 Loss - 1.3531653491636166\n",
      "Iteration - 500 Epoch - 99 Loss - 1.3548151284396768\n",
      "Iteration - 600 Epoch - 99 Loss - 1.3589569785432292\n",
      "Total validation loss 1.347769148818748 after 99 epochs\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    train(train_dataloader,epoch)\n",
    "    validation(val_dataloader,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 49\n",
      "18.367346938775512\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(testloader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        labels = torch.argmax(labels,dim =1)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(correct,total)\n",
    "print(100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir_path, test_key,valid_key, transform=None):\n",
    "        self.dir_path = dir_path\n",
    "        self.files = os.listdir(self.dir_path)\n",
    "        self.test_key = test_key\n",
    "        self.valid_key = valid_key\n",
    "        self.melspecs,self.Y = self.loadData(dir_path)\n",
    "        print(len(self.melspecs),len(self.Y))\n",
    "        \n",
    "    def loadData(self,dir_path):\n",
    "        files = os.listdir(dir_path)\n",
    "        train_keys = list((spk_id[i] for i in range(10) if i not in [self.test_key,self.valid_key]))\n",
    "        melspecs = []\n",
    "        Y = []\n",
    "        for key in train_keys:\n",
    "            for file in files:\n",
    "                if file[:2]==key:\n",
    "                    r, sr = librosa.load(dir_path + file, res_type='kaiser_fast')\n",
    "                    melspec = librosa.feature.melspectrogram(y=r, sr=sr,n_fft = 512, hop_length=160, win_length=320,n_mels=24)\n",
    "                    temp = self.chunk(torch.Tensor(melspec))\n",
    "                    melspecs.extend(temp)\n",
    "                    for _ in range(len(temp)):\n",
    "                        ye = torch.zeros(7,dtype = int)\n",
    "                        ye[emotion_code[file[5]]]=1\n",
    "                        yg = torch.zeros(2,dtype = int)\n",
    "                        yg[gender_code[file[:2]]]=1\n",
    "                        Y.append((ye,yg))\n",
    "        return melspecs,Y\n",
    "    \n",
    "    def chunk(self,melspec):\n",
    "        melspec = melspec.transpose(0,1)\n",
    "        res = []\n",
    "        for i in range(0,melspec.size(0),50):\n",
    "            temp = melspec[i:i+100,:]\n",
    "            if temp.size(0)==100:\n",
    "                res.append(temp)\n",
    "        return res        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.melspecs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.melspecs[idx],self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2629 2629\n"
     ]
    }
   ],
   "source": [
    "trmt = MTL_Dataset(\"EmoDB/wav/\",0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainmt_dataloader = DataLoader(trmt, batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTLVal_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir_path,valid_key, transform=None):\n",
    "        self.dir_path = dir_path\n",
    "        self.files = os.listdir(self.dir_path)\n",
    "        self.valid_key = valid_key\n",
    "        self.melspecs,self.Y = self.loadData(dir_path)\n",
    "        print(len(self.melspecs),len(self.Y))\n",
    "        \n",
    "    def loadData(self,dir_path):\n",
    "        files = os.listdir(dir_path)\n",
    "        melspecs = []\n",
    "        Y = []\n",
    "        for file in files:\n",
    "            if file[:2]==spk_id[self.valid_key]:\n",
    "                r, sr = librosa.load(dir_path + file, res_type='kaiser_fast')\n",
    "                melspec = librosa.feature.melspectrogram(y=r, sr=sr,n_fft = 512, hop_length=160, win_length=320,n_mels=24)\n",
    "                temp = self.chunk(torch.Tensor(melspec))\n",
    "                melspecs.extend(temp)\n",
    "                for _ in range(len(temp)):\n",
    "                    ye = torch.zeros(7,dtype = int)\n",
    "                    ye[emotion_code[file[5]]]=1\n",
    "                    yg = torch.zeros(2,dtype = int)\n",
    "                    yg[gender_code[file[:2]]]=1\n",
    "                    Y.append((ye,yg))\n",
    "        return melspecs,Y\n",
    "    \n",
    "    def chunk(self,melspec):\n",
    "        melspec = melspec.transpose(0,1)\n",
    "        res = []\n",
    "        for i in range(0,melspec.size(0),50):\n",
    "            temp = melspec[i:i+100,:]\n",
    "            if temp.size(0)==100:\n",
    "                res.append(temp)\n",
    "        return res        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.melspecs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.melspecs[idx],self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399 399\n"
     ]
    }
   ],
   "source": [
    "vmt = MTLVal_Dataset(\"EmoDB/wav/\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "validmt_dataloader = DataLoader(vmt, batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTLTest_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir_path,test_key, transform=None):\n",
    "        self.dir_path = dir_path\n",
    "        self.files = os.listdir(self.dir_path)\n",
    "        self.test_key = test_key\n",
    "        self.melspecs,self.Y = self.loadData(dir_path)\n",
    "        print(len(self.melspecs),len(self.Y))\n",
    "        \n",
    "    def loadData(self,dir_path):\n",
    "        files = os.listdir(dir_path)\n",
    "        melspecs = []\n",
    "        Y = []\n",
    "        for file in files:\n",
    "            if file[:2]==spk_id[self.test_key]:\n",
    "                r, sr = librosa.load(dir_path + file, res_type='kaiser_fast')\n",
    "                melspec = librosa.feature.melspectrogram(y=r, sr=sr,n_fft = 512, hop_length=160, win_length=320,n_mels=24)\n",
    "                melspec = melspec.transpose()\n",
    "                melspecs.append(melspec)\n",
    "                ye = torch.zeros(7,dtype = int)\n",
    "                ye[emotion_code[file[5]]]=1\n",
    "                yg = torch.zeros(2,dtype = int)\n",
    "                yg[gender_code[file[:2]]]=1\n",
    "                Y.append((ye,yg))\n",
    "        return melspecs,Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.melspecs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.melspecs[idx],self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    }
   ],
   "source": [
    "testmt = MTLTest_Dataset(\"EmoDB/wav/\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmt_dataloader = DataLoader(testmt, batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multitask(nn.Module):\n",
    "    def __init__(self, input_dim = 24, em_classes=7,gen_classes = 2):\n",
    "        super(multitask, self).__init__()\n",
    "        self.tdnn1 = TDNN(input_dim=input_dim, output_dim=512, context_size=5, dilation=1,dropout_p=0.5)\n",
    "        self.tdnn2 = TDNN(input_dim=512, output_dim=512, context_size=3, dilation=1,dropout_p=0.5)\n",
    "        self.tdnn3 = TDNN(input_dim=512, output_dim=512, context_size=2, dilation=2,dropout_p=0.5)\n",
    "        self.tdnn4 = TDNN(input_dim=512, output_dim=512, context_size=1, dilation=1,dropout_p=0.5)\n",
    "        self.tdnn5 = TDNN(input_dim=512, output_dim=512, context_size=1, dilation=3,dropout_p=0.5)\n",
    "        #### Frame levelPooling\n",
    "        self.segment6 = nn.Linear(1024, 512)\n",
    "        self.segment7 = nn.Linear(512, 512)\n",
    "        self.emotion = nn.Linear(512, em_classes)\n",
    "        self.gender = nn.Linear(512, gen_classes)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, inputs):\n",
    "        tdnn1_out = self.tdnn1(inputs)\n",
    "#         return tdnn1_out\n",
    "        tdnn2_out = self.tdnn2(tdnn1_out)\n",
    "        tdnn3_out = self.tdnn3(tdnn2_out)\n",
    "        tdnn4_out = self.tdnn4(tdnn3_out)\n",
    "        tdnn5_out = self.tdnn5(tdnn4_out)\n",
    "        ### Stat Pool\n",
    "        mean = torch.mean(tdnn5_out,1)\n",
    "        std = torch.std(tdnn5_out,1)\n",
    "        stat_pooling = torch.cat((mean,std),1)\n",
    "        segment6_out = self.segment6(stat_pooling)\n",
    "        x_vec = self.segment7(segment6_out)\n",
    "        em_predictions = self.emotion(x_vec)\n",
    "        gen_predictions = self.gender(x_vec)\n",
    "        return em_predictions,gen_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0098, -0.0171, -0.0305, -0.0194, -0.0344,  0.0426,  0.0026],\n",
      "        [-0.0093, -0.0194, -0.0295, -0.0188, -0.0358,  0.0425,  0.0020],\n",
      "        [-0.0084, -0.0177, -0.0309, -0.0175, -0.0381,  0.0416,  0.0036],\n",
      "        [-0.0101, -0.0168, -0.0309, -0.0189, -0.0360,  0.0429,  0.0039]],\n",
      "       grad_fn=<AddmmBackward>), tensor([[0.0078, 0.0154],\n",
      "        [0.0082, 0.0159],\n",
      "        [0.0083, 0.0154],\n",
      "        [0.0078, 0.0159]], grad_fn=<AddmmBackward>))\n"
     ]
    }
   ],
   "source": [
    "input_feats = [4,100,24]\n",
    "input = torch.rand(input_feats)\n",
    "mt_model = multitask()\n",
    "out = mt_model(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainmt(train_dataloader,epoch):\n",
    "    running_loss = 0.0\n",
    "    train_loss_list=[]\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad = True\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        labels[0] = torch.argmax(labels[0],dim =1)\n",
    "        labels[1] = torch.argmax(labels[1],dim =1)\n",
    "        # forward + backward + optimize\n",
    "        outputs = mt_model(inputs)\n",
    "        loss1 = criterion(outputs[0], labels[0])\n",
    "        loss2 = criterion(outputs[1], labels[1])\n",
    "        loss = loss1+loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_list.append(loss.item())\n",
    "        mean_loss = np.mean(np.asarray(train_loss_list))\n",
    "        if i%100==0:\n",
    "            print('Iteration - {} Epoch - {} Total training loss - {} '.format(i,epoch,mean_loss))\n",
    "            \n",
    "def validationmt(valid_dataloader,epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_list=[]\n",
    "        for i, data in enumerate(valid_dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            labels[0] = torch.argmax(labels[0],dim =1)\n",
    "            labels[1] = torch.argmax(labels[1],dim =1)\n",
    "            outputs = mt_model(inputs)\n",
    "            loss1 = criterion(outputs[0], labels[0])\n",
    "            loss2 = criterion(outputs[1], labels[1])\n",
    "            loss = loss1+loss2\n",
    "            val_loss_list.append(loss.item())\n",
    "            if i%100==0:\n",
    "                print('Iteration - {} Epoch - {} Loss - {}'.format(i,epoch,np.mean(np.asarray(val_loss_list))))\n",
    "                \n",
    "        mean_loss = np.mean(np.asarray(val_loss_list))\n",
    "        print('Total validation loss {} after {} epochs'.format(mean_loss,epoch))\n",
    "        model_save_path = os.path.join( 'best_check_point_'+str(epoch)+'_'+str(mean_loss))\n",
    "        state_dict = {'model': model.state_dict(),'optimizer': optimizer.state_dict(),'epoch': epoch}\n",
    "        torch.save(state_dict, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    trainmt(trainmt_dataloader,epoch)\n",
    "    validationmt(validmt_dataloader,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(testmt_dataloader):\n",
    "        inputs, labels = data\n",
    "        labels[0] = torch.argmax(labels[0],dim =1)\n",
    "        labels[1] = torch.argmax(labels[1],dim =1)\n",
    "        outputs = mt_model(inputs)\n",
    "        print(outputs,labels)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "# print(correct,total)\n",
    "# print(100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
