{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\MTP\n"
     ]
    }
   ],
   "source": [
    "cd F:\\MTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.listdir('EmoDB/wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract speaker embeddings(x-vectors) with a pretrained TDNN model using SpeechBrain from each utternace with a size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", savedir=\"pretrained_models/spkrec-xvect-voxceleb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each utterance from the database is name according to the scheme :\n",
    "    Positions 1-2: number of speaker\n",
    "    Positions 3-5: code for text\n",
    "    Position 6: emotion (sorry, letter stands for german emotion word)\n",
    "    Position 7: if there are more than two versions these are numbered a, b, c....\n",
    "\n",
    "Example: 03a01Fa.wav is the audio file from Speaker 03 speaking text a01 with\n",
    "the emotion \"Freude\" (Happiness in German).\n",
    "\n",
    "So now,we extract the emotion from the filenames and consider it as y in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_code = {\n",
    "    'W':0, #anger\n",
    "    'L':1, #boredom\n",
    "    'E':2, #disgust\n",
    "    'A':3, #fear\n",
    "    'F':4, #happy\n",
    "    'T':5, #sad\n",
    "    'N':6  #neutral\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vec_size = 512\n",
    "X = torch.zeros([len(path),x_vec_size])\n",
    "y = torch.zeros([len(path),7],dtype = int)\n",
    "for i in range(len(path)):\n",
    "    signal, fs =torchaudio.load('EmoDB/wav/'+path[i])\n",
    "    embeddings = classifier.encode_batch(signal)\n",
    "    X[i] = embeddings[0][0]\n",
    "    y[i][emotion_code[path[i][5]]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([535, 512]) torch.Size([535, 7])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert them into numpy arrays so that we can train them easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.numpy()\n",
    "y=y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_X=defaultdict(list)\n",
    "dict_y=defaultdict(list)\n",
    "for i in range(len(path)):\n",
    "    dict_X[path[i][:2]].append(X[i])\n",
    "    dict_y[path[i][:2]].append(y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this evaluation speaker independent, we separate the data speaker-wise and use leave-one-out validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {0:\"03\",1:\"08\",2:\"09\",3:\"10\",4:\"11\",5:\"12\",6:\"13\",7:\"14\",8:\"15\",9:\"16\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requirements\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the x-vectors are extracted before the last layer from a pretrained model, we just add softmax layer to get the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(X,y,X_test,y_test):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7, input_shape=(len(X[0]),)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    model.fit(X,y,batch_size=20, epochs=200)\n",
    "    predict_y = model.predict(X_test)\n",
    "    predicted_test = predict_y\n",
    "    np.where(np.argmax(predict_y[:]),1,0)\n",
    "    for i in range(len(predict_y)):\n",
    "        temp = np.argmax(predict_y[i])\n",
    "        predict_y[i] = np.zeros((1,7))\n",
    "        predict_y[i][temp] = 1\n",
    "    acc=accuracy_score(y_test,predict_y)\n",
    "    print(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp = 9\n",
    "acc = 0\n",
    "accuracies = [0]*10\n",
    "for i in range(10):\n",
    "    a = list((dict_X[dict[i]] for i in range(10) if i != temp))\n",
    "    b = list((dict_y[dict[i]] for i in range(10) if i != temp))\n",
    "    X = np.concatenate(a)\n",
    "    X_test = np.array(dict_X[dict[temp]])\n",
    "    y_test = np.array(dict_y[dict[temp]])\n",
    "    y = np.concatenate(b)\n",
    "    temp -= 1\n",
    "    print(X.shape,y.shape,X_test.shape,y_test.shape)\n",
    "    temp_acc = train_and_test(X,y,X_test,y_test)\n",
    "    accuracies[temp]=temp_acc\n",
    "    print(f'Accuracy for iteration: {temp_acc}')\n",
    "    acc += temp_acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leave-one-speaker-out validation accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5862068965517241,\n",
       " 0.7209302325581395,\n",
       " 0.7368421052631579,\n",
       " 0.6909090909090909,\n",
       " 0.6,\n",
       " 0.6885245901639344,\n",
       " 0.8840579710144928,\n",
       " 0.6964285714285714,\n",
       " 0.8450704225352113,\n",
       " 0.7959183673469388]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy :72.4488824777126%\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall accuracy :\"+ str(acc*10)+\"%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "M31_X,M31_y=np.array(dict_X[\"03\"]),np.array(dict_y[\"03\"])\n",
    "F34_X,F34_y=np.array(dict_X[\"08\"]),np.array(dict_y[\"08\"])\n",
    "F21_X,F21_y=np.array(dict_X[\"09\"]),np.array(dict_y[\"09\"])\n",
    "M32_X,M32_y=np.array(dict_X[\"10\"]),np.array(dict_y[\"10\"])\n",
    "M26_X,M26_y=np.array(dict_X[\"11\"]),np.array(dict_y[\"11\"])\n",
    "M30_X,M30_y=np.array(dict_X[\"12\"]),np.array(dict_y[\"12\"])\n",
    "F32_X,F32_y=np.array(dict_X[\"13\"]),np.array(dict_y[\"13\"])\n",
    "F35_X,F35_y=np.array(dict_X[\"14\"]),np.array(dict_y[\"14\"])\n",
    "M25_X,M25_y=np.array(dict_X[\"15\"]),np.array(dict_y[\"15\"])\n",
    "F31_X,F31_y=np.array(dict_X[\"16\"]),np.array(dict_y[\"16\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
