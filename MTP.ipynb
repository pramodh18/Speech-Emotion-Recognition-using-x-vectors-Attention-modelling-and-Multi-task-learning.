{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\MTP\n"
     ]
    }
   ],
   "source": [
    "cd F:\\MTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.listdir('EmoDB/wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract speaker embeddings(x-vectors) with a pretrained TDNN model using SpeechBrain from each utternace with a size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", savedir=\"pretrained_models/spkrec-xvect-voxceleb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each utterance from the database is name according to the scheme :\n",
    "    Positions 1-2: number of speaker\n",
    "    Positions 3-5: code for text\n",
    "    Position 6: emotion (sorry, letter stands for german emotion word)\n",
    "    Position 7: if there are more than two versions these are numbered a, b, c....\n",
    "\n",
    "Example: 03a01Fa.wav is the audio file from Speaker 03 speaking text a01 with\n",
    "the emotion \"Freude\" (Happiness in German).\n",
    "\n",
    "So now,we extract the emotion from the filenames and consider it as y in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emotion_code = {\n",
    "    'W':0, #anger\n",
    "    'L':1, #boredom\n",
    "    'E':2, #disgust\n",
    "    'A':3, #fear\n",
    "    'F':4, #happy\n",
    "    'T':5, #sad\n",
    "    'N':6  #neutral\n",
    "}\n",
    "\n",
    "speaker_code = {\n",
    "    '03':0,\n",
    "    '08':1,\n",
    "    '09':2,\n",
    "    '10':3,\n",
    "    '11':4,\n",
    "    '12':5,\n",
    "    '13':6,\n",
    "    '14':7,\n",
    "    '15':8,\n",
    "    '16':9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_vec_size = 512\n",
    "X = torch.zeros([len(path),x_vec_size])\n",
    "y = torch.zeros([len(path),7],dtype = int)\n",
    "for i in range(len(path)):\n",
    "    signal, fs =torchaudio.load('EmoDB/wav/'+path[i])\n",
    "    embeddings = classifier.encode_batch(signal)\n",
    "    X[i] = embeddings[0][0]\n",
    "    y[i][emotion_code[path[i][5]]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ys = torch.zeros([len(path),10],dtype = int)\n",
    "# yg = torch.zeros([len(path),2],dtype = int)\n",
    "# for i in range(len(path)):\n",
    "#     ys[i][speaker_code[path[i][:2]]]=1\n",
    "#     yg[i][gender_code[path[i][:2]]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(X.shape,y.shape,ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(ys[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert them into numpy arrays so that we can train them easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X=X.detach().numpy()\n",
    "y=y.detach().numpy()\n",
    "#ys=ys.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict_X=defaultdict(list)\n",
    "dict_y=defaultdict(list)\n",
    "#dict_ys=defaultdict(list)\n",
    "for i in range(len(path)):\n",
    "    dict_X[path[i][:2]].append(X[i])\n",
    "    dict_y[path[i][:2]].append(y[i])\n",
    "   # dict_ys[path[i][:2]].append(ys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X.shape,ys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this evaluation speaker independent, we separate the data speaker-wise and use leave-one-out validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict = {0:\"03\",1:\"08\",2:\"09\",3:\"10\",4:\"11\",5:\"12\",6:\"13\",7:\"14\",8:\"15\",9:\"16\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_code = {\n",
    "    '03':0,\n",
    "    '08':1,\n",
    "    '09':1,\n",
    "    '10':0,\n",
    "    '11':0,\n",
    "    '12':0,\n",
    "    '13':1,\n",
    "    '14':1,\n",
    "    '15':0,\n",
    "    '16':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#requirements\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the x-vectors are extracted before the last layer from a pretrained model, we just add softmax layer to get the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_and_test(X,y,X_test,y_test,X_valid,y_valid):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7, input_shape=(len(X[0]),)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    checkpoint_filepath = './tmp/checkpoint'\n",
    "    model_checkpoint_callback =ModelCheckpoint(filepath=checkpoint_filepath,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "    model.fit(X,y,batch_size=20, epochs=200,callbacks=model_checkpoint_callback,validation_data=(X_valid,y_valid))\n",
    "    loss,acc=model.evaluate(X_test,y_test)\n",
    "    print(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_and_test_speak(X,y,X_test,y_test,X_valid,y_valid):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_shape=(len(X[0]),)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    checkpoint_filepath = './tmp/checkpoint_speakers'\n",
    "    model_checkpoint_callback =ModelCheckpoint(filepath=checkpoint_filepath,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "    model.fit(X,y,batch_size=20, epochs=200,callbacks=model_checkpoint_callback,validation_data=(X_valid,y_valid))\n",
    "    loss,acc=model.evaluate(X_test,y_test)\n",
    "    print(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = 9\n",
    "acc = 0\n",
    "accuracies = [0]*10\n",
    "for i in range(10):\n",
    "    if temp == 0:\n",
    "        valid = 9\n",
    "    else:\n",
    "        valid = temp-1\n",
    "    \n",
    "    a = list((dict_X[dict[i]] for i in range(10) if i not in [temp,valid] ))\n",
    "    b = list((dict_y[dict[i]] for i in range(10) if i not in [temp,valid]))\n",
    "    X_ = np.concatenate(a)\n",
    "    X_test = np.array(dict_X[dict[temp]])\n",
    "    X_valid = np.array(dict_X[dict[valid]])\n",
    "    y_test = np.array(dict_y[dict[temp]])\n",
    "    y_valid = np.array(dict_y[dict[valid]])\n",
    "    y_ = np.concatenate(b)\n",
    "    temp -= 1\n",
    "    print(9-temp)\n",
    "    temp_acc = train_and_test(X_,y_,X_test,y_test,X_valid,y_valid)\n",
    "    accuracies[temp]=temp_acc\n",
    "    print(f'Accuracy for iteration: {temp_acc}')\n",
    "    acc += temp_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leave-one-speaker-out validation accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.568965494632721,\n",
       " 0.6744186282157898,\n",
       " 0.7631579041481018,\n",
       " 0.6545454263687134,\n",
       " 0.7428571581840515,\n",
       " 0.688524603843689,\n",
       " 0.6666666865348816,\n",
       " 0.6428571343421936,\n",
       " 0.8873239159584045,\n",
       " 0.8775510191917419]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy :71.66867971420288%\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall accuracy :\"+ str(acc*10)+\"%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model for speaker recognition\n",
    "# Xtrain1, Xtest, ytrain1, ytest = train_test_split(X.numpy(), ys.numpy(), test_size=0.3, random_state=42)\n",
    "# Xtrain, Xvalid, ytrain, yvalid = train_test_split(Xtrain1, ytrain1, test_size=0.15, random_state=42)\n",
    "# model = Sequential()\n",
    "# model.add(Dense(10, input_shape=(len(X[0]),)))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "# checkpoint_filepath = './tmp/checkpoint_speakers'\n",
    "# model_checkpoint_callback =ModelCheckpoint(filepath=checkpoint_filepath,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "# model.fit(Xtrain,ytrain,batch_size=20, epochs=200,callbacks=model_checkpoint_callback,validation_data=(Xvalid,yvalid))\n",
    "# loss,acc=model.evaluate(Xtest,ytest)\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for gender recognition\n",
    "# Xtrain1, Xtest, ytrain1, ytest = train_test_split(X.numpy(), yg.numpy(), test_size=0.3, random_state=42)\n",
    "# Xtrain, Xvalid, ytrain, yvalid = train_test_split(Xtrain1, ytrain1, test_size=0.15, random_state=42)\n",
    "# model = Sequential()\n",
    "# model.add(Dense(2, input_shape=(len(X[0]),)))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "# checkpoint_filepath = './tmp/checkpoint_speakers'\n",
    "# model_checkpoint_callback =ModelCheckpoint(filepath=checkpoint_filepath,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "# model.fit(Xtrain,ytrain,batch_size=20, epochs=200,callbacks=model_checkpoint_callback,validation_data=(Xvalid,yvalid))\n",
    "# loss,acc=model.evaluate(Xtest,ytest)\n",
    "# print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "M31_X,M31_y=np.array(dict_X[\"03\"]),np.array(dict_y[\"03\"])\n",
    "F34_X,F34_y=np.array(dict_X[\"08\"]),np.array(dict_y[\"08\"])\n",
    "F21_X,F21_y=np.array(dict_X[\"09\"]),np.array(dict_y[\"09\"])\n",
    "M32_X,M32_y=np.array(dict_X[\"10\"]),np.array(dict_y[\"10\"])\n",
    "M26_X,M26_y=np.array(dict_X[\"11\"]),np.array(dict_y[\"11\"])\n",
    "M30_X,M30_y=np.array(dict_X[\"12\"]),np.array(dict_y[\"12\"])\n",
    "F32_X,F32_y=np.array(dict_X[\"13\"]),np.array(dict_y[\"13\"])\n",
    "F35_X,F35_y=np.array(dict_X[\"14\"]),np.array(dict_y[\"14\"])\n",
    "M25_X,M25_y=np.array(dict_X[\"15\"]),np.array(dict_y[\"15\"])\n",
    "F31_X,F31_y=np.array(dict_X[\"16\"]),np.array(dict_y[\"16\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "melspecs =[0 for i in range(len(path))]\n",
    "Y = torch.zeros([len(path),7],dtype = int)\n",
    "for i in range(len(path)):\n",
    "    r, sr = librosa.load('EmoDB/wav/'+path[i], res_type='kaiser_fast')\n",
    "    melspec = librosa.feature.melspectrogram(y=r, sr=sr,n_fft = 512, hop_length=160, win_length=320,n_mels=24)\n",
    "    melspecs[i]=melspec\n",
    "    Y[i][emotion_code[path[i][5]]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfeats_X=defaultdict(list)\n",
    "testfeats_y=defaultdict(list)\n",
    "for i in range(len(path)):\n",
    "    testfeats_X[path[i][:2]].append(melspecs[i])\n",
    "    testfeats_y[path[i][:2]].append(Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeats_X=defaultdict(list)\n",
    "trainfeats_y=defaultdict(list)\n",
    "for j in range(len(path)):    \n",
    "    for i in range(0,len(melspecs[j][1]),50):\n",
    "        temp = melspecs[0][:,i:i+100]\n",
    "        if len(temp[1])==100:\n",
    "            trainfeats_X[path[j][:2]].append(melspecs[0][:,i:i+100])\n",
    "            trainfeats_y[path[j][:2]].append(Y[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=[]\n",
    "for j in range(10):    \n",
    "    for i in range(0,len(trainfeats_X[dict[j]])):\n",
    "        train_set.append((torch.tensor(trainfeats_X[dict[j]][i]),trainfeats_y[dict[j]][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapex(x):  \n",
    "    return torch.reshape(x,(1,24,x.shape[1]))\n",
    "def reshapey(x):  \n",
    "    return torch.reshape(x,(1,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn\n",
    "# import torchvision.datasets as datasets\n",
    "# import torchvision.transforms as transforms\n",
    "import speechbrain as sb\n",
    "from speechbrain.nnet.pooling import StatisticsPooling\n",
    "from speechbrain.nnet.CNN import Conv1d\n",
    "from speechbrain.nnet.linear import Linear\n",
    "from speechbrain.nnet.activations import Softmax\n",
    "from speechbrain.nnet.normalization import BatchNorm1d\n",
    "\n",
    "class Xvector(torch.nn.Module):\n",
    "    \"\"\"This model extracts X-vectors for speaker recognition and diarization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    device : str\n",
    "        Device used e.g. \"cpu\" or \"cuda\".\n",
    "    activation : torch class\n",
    "        A class for constructing the activation layers.\n",
    "    tdnn_blocks : int\n",
    "        Number of time-delay neural (TDNN) layers.\n",
    "    tdnn_channels : list of ints\n",
    "        Output channels for TDNN layer.\n",
    "    tdnn_kernel_sizes : list of ints\n",
    "        List of kernel sizes for each TDNN layer.\n",
    "    tdnn_dilations : list of ints\n",
    "        List of dilations for kernels in each TDNN layer.\n",
    "    lin_neurons : int\n",
    "        Number of neurons in linear layers.\n",
    "    Example\n",
    "    -------\n",
    "    >>> compute_xvect = Xvector('cpu')\n",
    "    >>> input_feats = torch.rand([5, 10, 40])\n",
    "    >>> outputs = compute_xvect(input_feats)\n",
    "    >>> outputs.shape\n",
    "    torch.Size([5, 1, 512])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        activation,\n",
    "        tdnn_blocks,\n",
    "        tdnn_channels,\n",
    "        tdnn_kernel_sizes,\n",
    "        tdnn_dilations,\n",
    "        lin_neurons,\n",
    "        in_channels,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        # TDNN layers\n",
    "        for block_index in range(tdnn_blocks):\n",
    "            out_channels = tdnn_channels[block_index]\n",
    "            self.blocks.extend(\n",
    "                [\n",
    "                    Conv1d(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        kernel_size=tdnn_kernel_sizes[block_index],\n",
    "                        dilation=tdnn_dilations[block_index],\n",
    "                    ),\n",
    "                    activation(),\n",
    "                    BatchNorm1d(input_size=out_channels),\n",
    "                ]\n",
    "            )\n",
    "            in_channels = tdnn_channels[block_index]\n",
    "\n",
    "        # Statistical pooling\n",
    "        self.blocks.append(StatisticsPooling())\n",
    "\n",
    "        # Final linear transformation\n",
    "        self.blocks.append(\n",
    "            Linear(\n",
    "                input_size=out_channels * 2,\n",
    "                n_neurons=lin_neurons,\n",
    "                bias=True,\n",
    "                combine_dims=False,\n",
    "            )\n",
    "        )\n",
    "        self.blocks.append(Linear(7,input_size=lin_neurons))\n",
    "#         self.blocks.append(Softmax())\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x, lens=None):\n",
    "        \"\"\"Returns the x-vectors.\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.blocks:\n",
    "            try:\n",
    "                x = layer(x, lengths=lens)\n",
    "            except TypeError:\n",
    "                x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_xvect = Xvector(\"cpu\",torch.nn.LeakyReLU,5,[512, 512, 512, 512, 1500],[5, 3, 3, 1, 1],[1, 2, 3, 1, 1],512,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(compute_xvect.parameters(), lr = 0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for data in train_set[:1840]:    \n",
    "        \n",
    "        X, y = data\n",
    "        compute_xvect.zero_grad()\n",
    "        output = compute_xvect(reshapex(X))\n",
    "        y=reshapey(y)\n",
    "        loss=criterion(output[0], torch.argmax(y).view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 75.33333333333333\n"
     ]
    }
   ],
   "source": [
    "corr = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_set[1840:2140]:\n",
    "        X,y=data\n",
    "        output = compute_xvect(reshapex(X))\n",
    "        for idx,i in enumerate(output):\n",
    "            if torch.argmax(i)==y[idx]:\n",
    "                corr+=1\n",
    "            total+=1\n",
    "            \n",
    "print(\"Accuracy = \"+str(corr*100/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
